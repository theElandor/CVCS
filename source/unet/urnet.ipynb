{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels: int, out_channels: int, padding=0):\n",
    "\treturn nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=padding)\n",
    "\n",
    "\n",
    "def max_pool_2d():\n",
    "\treturn nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncodeLayer(nn.Module):\n",
    "    # just a standard convolution layer.\n",
    "\tdef __init__(self, in_channels: int, out_channels: int, activated=True,max_pool=False, padding=0):\n",
    "\t\tsuper(UnetEncodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "            conv3x3(in_channels, out_channels, padding=padding),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "        ]\n",
    "\t\tif activated:\n",
    "\t\t\tlayers += [nn.ReLU()]\n",
    "\t\tif max_pool:\n",
    "\t\t\tlayers += [max_pool_2d()]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\t\n",
    "class UnetUpscaleLayer(nn.Module):\n",
    "\tdef __init__(self, scale_factor, in_channels):\n",
    "\t\tsuper(UnetUpscaleLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tnn.Upsample(scale_factor = (scale_factor,scale_factor), mode = 'bilinear'),\n",
    "\t\t\tconv3x3(in_channels, in_channels//2,padding=1)\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\n",
    "class UnetForwardDecodeLayer(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, padding=0):\n",
    "\t\tsuper(UnetForwardDecodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tconv3x3(in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tconv3x3(in_channels=out_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Urnet(nn.Module):\t\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Urnet, self).__init__()\n",
    "\t\tself.residuals = []\n",
    "    \t# encoding part of the Unet vanilla architecture\n",
    "\t\tself.encode1 = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=1),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=1), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode3 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=1),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode4 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=1),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode5 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=1),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=1),\n",
    "\t\t)\n",
    "\t\tself.upscale1 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 1024)\n",
    "\t\t)\n",
    "\t\tself.decode_forward1 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(1024,512, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale2 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 512)\n",
    "\t\t)\n",
    "\t\tself.decode_forward2 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(512, 256, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale3 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,256)\n",
    "\t\t)\n",
    "\t\tself.decode_forward3 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(256,128,padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale4 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,128)\n",
    "\t\t)\n",
    "\t\tself.decode_forward4 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(128,64, padding=1),\n",
    "\t\t\tnn.Conv2d(64, 6, kernel_size=1) # final conv 1x1\n",
    "\t\t\t# Model output is 6xHxW, so we have a prob. distribution\n",
    "\t\t\t# for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "\t\t)\t\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\tself.x1 = self.encode1(x)\n",
    "\t\tself.x2 = self.encode2(self.x1)\n",
    "\t\tself.x3 = self.encode3(self.x2)\n",
    "\t\tself.x4 = self.encode4(self.x3)\n",
    "\t\tself.x5 = self.encode5(self.x4)\n",
    "\n",
    "\t\ty1 = self.upscale1(self.x5)\n",
    "\t\tc1 = torch.concat((self.x4, y1), 1)\n",
    "\t\ty2 = self.decode_forward1(c1)\n",
    "\t\t\n",
    "\t\ty2 = self.upscale2(y2)\n",
    "\t\tc2 = torch.concat((self.x3, y2), 1)\n",
    "\t\ty3 = self.decode_forward2(c2)\n",
    "\n",
    "\t\ty3 = self.upscale3(y3)\n",
    "\t\tc3 = torch.concat((functional.center_crop(y3, 150), self.x2), 1)\n",
    "\t\ty4 = self.decode_forward3(c3)\n",
    "\n",
    "\t\ty4 = self.upscale4(y4)\n",
    "\t\tc4 = torch.concat((self.x1, y4), 1)\n",
    "\t\tsegmap = self.decode_forward4(c4)\n",
    "\t\treturn segmap\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PostDamDataset(Dataset):\n",
    "\tdef __init__(self, img_dir, masks_dir):\n",
    "\t\tself.idir = img_dir\n",
    "\t\tself.mdir = masks_dir\n",
    "\t\tself.data = {} # index : (image, mask)\n",
    "\t\tself.color_to_label = {\n",
    "\t\t\t(1, 1, 0): 0,  # Yellow (cars)\n",
    "\t\t\t(0, 1, 0): 1, # Green (trees)\n",
    "\t\t\t(0, 0, 1): 2, # Blue (buildings)\n",
    "\t\t\t(1, 0, 0): 3,  # Red (clutter)\n",
    "\t\t\t(1, 1, 1): 4, # White(impervious surface),\n",
    "\t\t\t(0, 1, 1): 5 # Aqua (low vegetation)\n",
    "    \t}\n",
    "\t\t# for index in range(self.__len__()):\n",
    "\t\t# \timg_path = os.path.join(self.idir, \"Image_{}.tif\".format(index))\n",
    "\t\t# \tmask_path = os.path.join(self.mdir, \"Label_{}.tif\".format(index))\n",
    "\t\t# \ttif_img = Image.open(img_path)\n",
    "\t\t# \ttif_mask = Image.open(mask_path)\n",
    "\t\t# \tself.data[index] = (ToTensor()(tif_img), self.convert(ToTensor()(tif_mask)))\t\t\t\n",
    "\tdef convert(self,mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the RGB (3x300x300) mask into a \n",
    "\t\t'class label mask' needed when computing the loss function.\n",
    "\t\tIn this new representation for each pixel we have a value\n",
    "\t\tbetween [0,C) where C is the number of classes, so 6 in this case.\n",
    "\t\tThis new tensor will have shape 1x300x300.\n",
    "\t\t\"\"\"\t\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys()))\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values()))    \n",
    "\t\treshaped_mask = mask.permute(1, 2, 0).reshape(-1, 3)\n",
    "\t\tclass_label_mask = torch.zeros(reshaped_mask.shape[0], dtype=torch.long)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (reshaped_mask == color).all(dim=1)\n",
    "\t\t\tclass_label_mask[match] = label\n",
    "\t\tclass_label_mask = class_label_mask.reshape(mask.shape[1], mask.shape[2])\t\t\n",
    "\t\treturn class_label_mask\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\titems = os.listdir(self.idir)\n",
    "\t\tfiles = [item for item in items if os.path.isfile(os.path.join(self.idir, item))]\n",
    "\t\treturn len(files)\t\t\n",
    "\tdef __getitem__(self, idx):\t\t\t\n",
    "\t\timg_path = os.path.join(self.idir, \"Image_{}.tif\".format(idx))\n",
    "\t\tmask_path = os.path.join(self.mdir, \"Label_{}.tif\".format(idx))\n",
    "\t\ttif_img = Image.open(img_path)\n",
    "\t\ttif_mask = Image.open(mask_path)\t\t\n",
    "\t\treturn (ToTensor()(tif_img), self.convert(ToTensor()(tif_mask)), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network on NVIDIA GeForce GTX 1060 6GB\n",
      "Number of parameters : 34525446\n",
      "Dataset length: 2400\n"
     ]
    }
   ],
   "source": [
    "#dataset = PostDamDataset(\"/content/drive/MyDrive/Postdam/Images\", \"/content/drive/MyDrive/Postdam/Labels\")\n",
    "dataset = PostDamDataset(\"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Images\", \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Labels\")\n",
    "assert torch.cuda.is_available(), \"Notebook is not configured properly!\"\n",
    "device = 'cuda:0'\n",
    "print(\"Training network on {}\".format(torch.cuda.get_device_name(device=device)))\n",
    "net = Urnet().to(device)  # instantiate your net\n",
    "num_params = sum([np.prod(p.shape) for p in net.parameters()])\n",
    "print(f\"Number of parameters : {num_params}\")\n",
    "print(\"Dataset length: {}\".format(dataset.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZATION\n",
    "batch_size = 4\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_sampler)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "#opt = optim.RMSprop(net.parameters(), lr = 0.001, momentum=0.99, weight_decay=1e-6)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL\n",
    "net = torch.load(\"C:\\\\Users\\\\eros\\\\Desktop\\\\Models\\\\urnet1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP\n",
    "epochs = 50\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    tot = 0\n",
    "    pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch}')\n",
    "    net.train()\n",
    "    for batch_index, (image, mask, _) in enumerate(train_loader):\n",
    "        tot+=1\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        mask_pred = net(image)\n",
    "        loss = crit(mask_pred, mask)\n",
    "        cumulative_loss += loss.item()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'Loss': cumulative_loss/tot})\n",
    "    pbar.close()\n",
    "    loss_values.append(cumulative_loss/tot)\n",
    "print(\"Training Done!\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_loader.dataset))\n",
    "with torch.no_grad():\n",
    "    net.eval()    \n",
    "    for i, (x,y, index) in enumerate(validation_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = net(x)\n",
    "        x_ref = x.cpu()\n",
    "        y_pred = y_pred.squeeze().cpu()\n",
    "        _,pred_mask = torch.max(y_pred, dim=0)\n",
    "        fig ,axarr = plt.subplots(1,3)\n",
    "        _,target_transformed_mask,_ = dataset.__getitem__(index.item())\n",
    "        axarr[0].imshow(x_ref.squeeze().swapaxes(0,2).swapaxes(0,1))\n",
    "        axarr[1].imshow(pred_mask)\n",
    "        axarr[2].imshow(target_transformed_mask)\n",
    "        plt.savefig(\"C:\\\\Users\\\\eros\\\\Desktop\\\\Models\\\\Output\\\\o{}.png\".format(i))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADER TEST\n",
    "#image, mask,idx = next(iter(train_loader))\n",
    "image, mask,idx = next(iter(validation_loader))\n",
    "print(idx)\n",
    "fig ,axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(image.squeeze().swapaxes(0,2).swapaxes(0,1))\n",
    "axarr[1].imshow(mask.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"C:\\\\Users\\\\eros\\\\Desktop\\\\Models\\\\urnet1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
