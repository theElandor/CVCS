{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels: int, out_channels: int, padding=0):\n",
    "\treturn nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=padding)\n",
    "\n",
    "\n",
    "def max_pool_2d():\n",
    "\treturn nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net most important blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncodeLayer(nn.Module):\n",
    "    # just a standard convolution layer.\n",
    "\tdef __init__(self, in_channels: int, out_channels: int, activated=True,max_pool=False, padding=0):\n",
    "\t\tsuper(UnetEncodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "            conv3x3(in_channels, out_channels, padding=padding),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "        ]\n",
    "\t\tif activated:\n",
    "\t\t\tlayers += [nn.ReLU()]\n",
    "\t\tif max_pool:\n",
    "\t\t\tlayers += [max_pool_2d()]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\t\n",
    "class UnetUpscaleLayer(nn.Module):\n",
    "\tdef __init__(self, scale_factor, in_channels):\n",
    "\t\tsuper(UnetUpscaleLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tnn.Upsample(scale_factor = (scale_factor,scale_factor), mode = 'bilinear'),\n",
    "\t\t\tconv3x3(in_channels, in_channels//2,padding=1)\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\n",
    "class UnetForwardDecodeLayer(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, padding=0):\n",
    "\t\tsuper(UnetForwardDecodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tconv3x3(in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tconv3x3(in_channels=out_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net base structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Urnet(nn.Module):\t\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Urnet, self).__init__()\n",
    "\t\tself.residuals = []\n",
    "    \t# encoding part of the Unet vanilla architecture\n",
    "\t\tself.encode1 = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=1),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=1), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode3 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=1),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode4 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=1),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode5 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=1),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=1),\n",
    "\t\t)\n",
    "\t\tself.upscale1 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 1024)\n",
    "\t\t)\n",
    "\t\tself.decode_forward1 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(1024,512, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale2 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 512)\n",
    "\t\t)\n",
    "\t\tself.decode_forward2 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(512, 256, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale3 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,256)\n",
    "\t\t)\n",
    "\t\tself.decode_forward3 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(256,128,padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale4 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,128)\n",
    "\t\t)\n",
    "\t\tself.decode_forward4 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(128,64, padding=1),\n",
    "\t\t\tnn.Conv2d(64, 6, kernel_size=1) # final conv 1x1\n",
    "\t\t\t# Model output is 6xHxW, so we have a prob. distribution\n",
    "\t\t\t# for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "\t\t)\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\tself.x1 = self.encode1(x)\n",
    "\t\tself.x2 = self.encode2(self.x1)\n",
    "\t\tself.x3 = self.encode3(self.x2)\n",
    "\t\tself.x4 = self.encode4(self.x3)\n",
    "\t\tself.x5 = self.encode5(self.x4)\n",
    "\n",
    "\t\ty1 = self.upscale1(self.x5)\n",
    "\t\tc1 = torch.concat((self.x4, y1), 1)\n",
    "\t\ty2 = self.decode_forward1(c1)\n",
    "\t\t\n",
    "\t\ty2 = self.upscale2(y2)\n",
    "\t\tc2 = torch.concat((self.x3, y2), 1)\n",
    "\t\ty3 = self.decode_forward2(c2)\n",
    "\n",
    "\t\ty3 = self.upscale3(y3)\n",
    "\t\tc3 = torch.concat((functional.center_crop(y3, 150), self.x2), 1)\n",
    "\t\ty4 = self.decode_forward3(c3)\n",
    "\n",
    "\t\ty4 = self.upscale4(y4)\n",
    "\t\tc4 = torch.concat((self.x1, y4), 1)\n",
    "\t\tsegmap = self.decode_forward4(c4)\n",
    "\t\treturn segmap\n",
    "\n",
    "net = Urnet()\n",
    "x = torch.rand((1,3,300,300))\n",
    "output = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Base Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\t# I leave it here but it's not needed if we use convolutional tokenizer.\n",
    "\tdef __init__(self, D, num_patches):\n",
    "\t\tsuper(PositionalEncoding, self).__init__()\n",
    "\t\tself.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, D))\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x + self.pos_embedding\n",
    "\t\n",
    "class VisionTransformer(nn.Module):\n",
    "\t# D = embedding dimension (patch is p*p*3 and will be projected to be D dimensional)\n",
    "\t# N = number of patches\n",
    "\t# p = patch size\t\n",
    "\tdef __init__(self, D, num_heads):\n",
    "\t\tsuper(VisionTransformer, self).__init__()\n",
    "\t\t# self.linear_projection = nn.Linear(p*p*3, D) don't need it in this architecture\n",
    "\t\t# self.positional_encoding = PositionalEncoding(D, N) neither this.\n",
    "\t\tself.layer_norm1 = nn.LayerNorm(D)\n",
    "\t\tself.layer_norm2 = nn.LayerNorm(D)\n",
    "\t\tself.MHA = nn.MultiheadAttention(embed_dim=D, num_heads=num_heads, batch_first=True)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\t# using D*4 hidden size according to original vision transformer paper\n",
    "\t\t\tnn.Linear(D, D*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D*4, D)\n",
    "\t\t)\n",
    "\t\t# we should have one of this for each head\n",
    "\tdef forward(self, x):\n",
    "\t\t#x = self.linear_projection(x) # N, p*p*3 --> N, D\n",
    "\t\t#self.r1 = self.positional_encoding(x) # add positional encoding to x, embedded patches\n",
    "\t\tself.r1 = x\t\t\n",
    "\t\tx = self.layer_norm1(self.r1)\t\t\n",
    "\t\tx = self.MHA(x,x,x)[0]\n",
    "\t\tself.r2 = x + self.r1\n",
    "\t\tx = self.layer_norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\treturn x + self.r2\n",
    "\n",
    "def vision_transformer(D,num_heads):\n",
    "    return VisionTransformer(D,num_heads)\n",
    "    \n",
    "class VisionTransformerEncoder(nn.Module):    \n",
    "    def __init__(self, D, num_heads, layers):\n",
    "        super(VisionTransformerEncoder, self).__init__()\n",
    "        self.layers =[vision_transformer(D,num_heads) for _ in range(layers)]\n",
    "        self.stack = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunet(Transformer + Unet) base structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tunet(nn.Module): # Unet + vision transformer\n",
    "    def __init__(self, d, heads, layers):\n",
    "        super(Tunet, self).__init__()\n",
    "        self.d = d\n",
    "        self.heads = heads\n",
    "        self.layers = layers\n",
    "        self.residuals = []        \n",
    "        # encoding part of the Unet vanilla architecture\n",
    "        self.encode1 = nn.Sequential(\n",
    "            UnetEncodeLayer(3, d//16, padding=1),\n",
    "        )\n",
    "        self.encode2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            UnetEncodeLayer(d//16, d//8, padding=1),            \n",
    "        )\n",
    "        self.encode3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            UnetEncodeLayer(d//8, d//4, padding=1),            \n",
    "        )\n",
    "        self.encode4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            UnetEncodeLayer(d//4, d//2, padding=1),\n",
    "        )\n",
    "        self.encode5 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),            \n",
    "            UnetEncodeLayer(d//2, d, padding=1),\n",
    "        )\n",
    "        self.transformer = VisionTransformerEncoder(self.d,self.heads,self.layers)\n",
    "        self.upscale1 = nn.Sequential(\n",
    "            UnetUpscaleLayer(2, d)\n",
    "        )\n",
    "        self.decode_forward1 = nn.Sequential(\n",
    "            UnetForwardDecodeLayer(d,d//2, padding=1)\n",
    "        )\n",
    "        self.upscale2 = nn.Sequential(\n",
    "            UnetUpscaleLayer(2, d//2)\n",
    "        )\n",
    "        self.decode_forward2 = nn.Sequential(\n",
    "            UnetForwardDecodeLayer(d//2, d//4, padding=1)\n",
    "        )\n",
    "        self.upscale3 = nn.Sequential(\n",
    "            UnetUpscaleLayer(2,d//4)\n",
    "        )\n",
    "        self.decode_forward3 = nn.Sequential(\n",
    "            UnetForwardDecodeLayer(d//4,d//8,padding=1)\n",
    "        )\n",
    "        self.upscale4 = nn.Sequential(\n",
    "            UnetUpscaleLayer(2,d//8)\n",
    "        )\n",
    "        self.decode_forward4 = nn.Sequential(\n",
    "            UnetForwardDecodeLayer(d//8,d//16, padding=1),\n",
    "            nn.Conv2d(d//16, 6, kernel_size=1) # final conv 1x1\n",
    "            # Model output is 6xHxW, so we have a prob. distribution\n",
    "            # for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        self.x1 = self.encode1(x)\n",
    "        self.x2 = self.encode2(self.x1)\n",
    "        self.x3 = self.encode3(self.x2)\n",
    "        self.x4 = self.encode4(self.x3)       \n",
    "        self.x5 = self.encode5(self.x4)        \n",
    "        _, _,h,w = self.x5.shape\n",
    "        self.sequence = self.x5.reshape(-1,h*w, self.d)\n",
    "        attention_encoded = self.transformer(self.sequence)\n",
    "        attention_encoded = attention_encoded.reshape(-1,self.d,h,w)\n",
    "        y1 = self.upscale1(attention_encoded)\n",
    "        c1 = torch.concat((self.x4, y1), 1)\n",
    "        y2 = self.decode_forward1(c1)\n",
    "\n",
    "        y2 = self.upscale2(y2)\n",
    "        c2 = torch.concat((self.x3, y2), 1)\n",
    "        y3 = self.decode_forward2(c2)\n",
    "\n",
    "        y3 = self.upscale3(y3)\n",
    "        c3 = torch.concat((functional.center_crop(y3, 150), self.x2), 1)\n",
    "        y4 = self.decode_forward3(c3)\n",
    "\n",
    "        y4 = self.upscale4(y4)\n",
    "        c4 = torch.concat((self.x1, y4), 1)\n",
    "        segmap = self.decode_forward4(c4)\n",
    "        return segmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converter\n",
    "+ class containing some utils to convert segmentations masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter:\n",
    "\tdef __init__(self):\n",
    "\t\tself.color_to_label = {\n",
    "            (1, 1, 0): 0,  # Yellow (cars)\n",
    "            (0, 1, 0): 1, # Green (trees)\n",
    "            (0, 0, 1): 2, # Blue (buildings)\n",
    "            (1, 0, 0): 3,  # Red (clutter)\n",
    "            (1, 1, 1): 4, # White(impervious surface),\n",
    "            (0, 1, 1): 5 # Aqua (low vegetation)\n",
    "        }\n",
    "\tdef iconvert(self, mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the class label mask needed by CrossEntropy Function\n",
    "\t\tto the original mask.\n",
    "\t\tinput: class label mask, HxW\n",
    "\t\toutput: original mask, HxWx3\n",
    "\t\t\"\"\"\n",
    "\t\tH,W = mask.shape\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys())).type(torch.float64)\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values())).type(torch.float64)\n",
    "\t\toutput = torch.ones(H,W,3).type(torch.float64)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (mask == label)\n",
    "\t\t\toutput[match] = color\n",
    "\t\treturn output\n",
    "\tdef convert(self,mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the RGB (Nx3x300x300) mask into a \n",
    "\t\t'class label mask' needed when computing the loss function.\n",
    "\t\tIn this new representation for each pixel we have a value\n",
    "\t\tbetween [0,C) where C is the number of classes, so 6 in this case.\n",
    "\t\tThis new tensor will have shape Nx300x300.\n",
    "\t\t\"\"\"\t\t\t\n",
    "\t\tC,H,W = mask.shape\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys()))\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values()))\n",
    "\t\treshaped_mask = mask.permute(1, 2, 0).reshape(-1, 3)\n",
    "\t\tclass_label_mask = torch.zeros(reshaped_mask.shape[0], dtype=torch.long)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (reshaped_mask == color.type(torch.float64)).all(dim=1)\n",
    "\t\t\tclass_label_mask[match] = label\n",
    "\t\tclass_label_mask = class_label_mask.reshape(H,W)\t\t\n",
    "\t\treturn class_label_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as v2\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "class PostDamDataset(Dataset):\n",
    "\tdef __init__(self, img_dir, masks_dir, extension,transforms=None):\n",
    "\t\tself.idir = img_dir\n",
    "\t\tself.mdir = masks_dir\t\t\n",
    "\t\tself.transforms = transforms\t\t\n",
    "\t\tself.extension = extension\n",
    "\t\tself.items = os.listdir(self.idir)\n",
    "\t\tself.files = [item for item in self.items if os.path.isfile(os.path.join(self.idir, item))]\n",
    "\t\tself.c = Converter()\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.files)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = os.path.join(self.idir, \"Image_{}{}\".format(idx, self.extension))\n",
    "\t\tmask_path = os.path.join(self.mdir, \"Label_{}{}\".format(idx, self.extension))\n",
    "\t\ttif_img = Image.open(img_path)\n",
    "\t\ttif_mask = Image.open(mask_path)\n",
    "\t\tif self.transforms:  # if transforms are provided, apply them\n",
    "\t\t\tfinal_image = self.transforms(ToTensor()(tif_img))\n",
    "\t\t# no transform is applied on mask obv.\n",
    "\t\telse:\n",
    "\t\t\tfinal_image = ToTensor()(tif_img)\n",
    "\t\treturn (final_image, self.c.convert(ToTensor()(tif_mask)), idx)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.GaussianBlur(kernel_size=(15), sigma=5),\n",
    "    v2.ElasticTransform(alpha=200.0)    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "1) Must define path of images and labels;\n",
    "2) Must define extension used for both images and labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network on NVIDIA GeForce GTX 1060 6GB\n",
      "Number of parameters : 54876246\n",
      "Dataset length: 30400\n"
     ]
    }
   ],
   "source": [
    "#dataset = PostDamDataset(\"/content/drive/MyDrive/Postdam/Images\", \"/content/drive/MyDrive/Postdam/Labels\")\n",
    "# Here you need to specify dataset path for both images and labels.\n",
    "# WARNING: dataset is pre-loaded in memory, so high RAM usage is expected.\n",
    "images_path = \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Postdam_300x300_full\\\\Images\"\n",
    "labels_path = \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Postdam_300x300_full\\\\Labels\"\n",
    "extension = \".png\"\n",
    "\n",
    "base_dataset = PostDamDataset(images_path, labels_path, extension)\n",
    "augmented_dataset = PostDamDataset(images_path, labels_path,extension, transforms=transforms)\n",
    "dataset = ConcatDataset([base_dataset, augmented_dataset])\n",
    "\n",
    "assert torch.cuda.is_available(), \"Notebook is not configured properly!\"\n",
    "device = 'cuda:0'\n",
    "print(\"Training network on {}\".format(torch.cuda.get_device_name(device=device)))\n",
    "net = Tunet(768, 12, 6).to(device)\n",
    "num_params = sum([np.prod(p.shape) for p in net.parameters()])\n",
    "print(f\"Number of parameters : {num_params}\")\n",
    "print(\"Dataset length: {}\".format(dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset split(augmented): 24320\n",
      "Validation dataset split: 3040\n",
      "Validation dataset split(only noise): 3040\n"
     ]
    }
   ],
   "source": [
    "#Dataset train/validation split according to validation split and seed.\n",
    "batch_size = 4\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "base_indices = list(range(dataset_size//2))\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(base_indices)\n",
    "augmented_indices = [i+(len(dataset)//2) for i in base_indices] # take coresponding augmented images\n",
    "split = int(np.floor((1-validation_split) * (dataset_size//2)))\n",
    "\n",
    "train_indices = base_indices[:split]+augmented_indices[:split]\n",
    "val_base_indices = base_indices[split:]\n",
    "val_noisy_indices = augmented_indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_base_sampler = SubsetRandomSampler(val_base_indices)\n",
    "valid_noisy_sampler = SubsetRandomSampler(val_noisy_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "#for validation loader batch size is default, so 1.\n",
    "validation_base_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_base_sampler)\n",
    "validation_noisy_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_noisy_sampler)\n",
    "\n",
    "print(f\"Train dataset split(augmented): {len(train_indices)}\")\n",
    "print(f\"Validation dataset split: {len(val_base_indices)}\")\n",
    "print(f\"Validation dataset split(only noise): {len(val_noisy_indices)}\")\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score as jsc\n",
    "def eval_model(net, validation_loader, validation_len, show_progress = False, write_output=False, prefix=\"\"):\n",
    "    # returns (macro, weighted) IoU\n",
    "    c = Converter()\n",
    "    macro = 0\n",
    "    weighted = 0\n",
    "    if show_progress:\n",
    "        pbar = tqdm(total=validation_len)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for i, (x,y, index) in enumerate(validation_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = net(x)\n",
    "            x_ref = x.cpu()\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            _,pred_mask = torch.max(y_pred, dim=0)\n",
    "\n",
    "            prediction = pred_mask.cpu().numpy().reshape(-1)\n",
    "            target = y.cpu().numpy().reshape(-1)        \n",
    "            weighted += jsc(target,prediction, average='weighted') # takes into account label imbalance\n",
    "            macro += jsc(target,prediction, average='macro') # simple mean over each class.            \n",
    "            if(write_output):\n",
    "                fig ,axarr = plt.subplots(1,3)\n",
    "                _,target_transformed_mask,_ = dataset.__getitem__(index.item())\n",
    "\n",
    "                axarr[0].title.set_text('Original Image')\n",
    "                axarr[0].imshow(x_ref.squeeze().swapaxes(0,2).swapaxes(0,1))\n",
    "\n",
    "                axarr[1].title.set_text('Model Output')\n",
    "                axarr[1].imshow(c.iconvert(pred_mask))\n",
    "\n",
    "                axarr[2].title.set_text('Original Mask')\n",
    "                axarr[2].imshow(c.iconvert(target_transformed_mask))\n",
    "                plt.savefig(\"output\\\\{}_Image{}.png\".format(prefix, i))\n",
    "                plt.close(fig)\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "    macro_score = macro / validation_len\n",
    "    weighted_score = weighted / validation_len\n",
    "    if show_progress:\n",
    "        pbar.close()\n",
    "    if(write_output):\n",
    "        print(\"Macro IoU score: {}\".format(macro_score))        \n",
    "        print(\"Weigthed IoU score: {}\".format(weighted_score))\n",
    "    return macro_score, weighted_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "+ Before launching, make sure to define the local directory where checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6080/6080 [1:05:57<00:00,  1.54it/s, Loss=1.13]\n",
      "Epoch 1: 100%|██████████| 6080/6080 [1:05:44<00:00,  1.54it/s, Loss=1.02]\n",
      "Epoch 2: 100%|██████████| 6080/6080 [1:05:28<00:00,  1.55it/s, Loss=0.947]\n",
      "Epoch 3: 100%|██████████| 6080/6080 [1:05:50<00:00,  1.54it/s, Loss=0.89] \n",
      "Epoch 4: 100%|██████████| 6080/6080 [1:05:07<00:00,  1.56it/s, Loss=0.841]\n",
      "Epoch 5: 100%|██████████| 6080/6080 [1:05:07<00:00,  1.56it/s, Loss=0.803]\n",
      "Epoch 6: 100%|██████████| 6080/6080 [1:05:06<00:00,  1.56it/s, Loss=0.759]\n",
      "Epoch 7: 100%|██████████| 6080/6080 [1:05:06<00:00,  1.56it/s, Loss=0.704]\n",
      "Epoch 8: 100%|██████████| 6080/6080 [1:05:07<00:00,  1.56it/s, Loss=0.67] \n",
      "Epoch 9: 100%|██████████| 6080/6080 [1:05:07<00:00,  1.56it/s, Loss=0.62] \n",
      "Epoch 10: 100%|██████████| 6080/6080 [1:05:51<00:00,  1.54it/s, Loss=0.582]\n",
      "Epoch 11: 100%|██████████| 6080/6080 [1:05:21<00:00,  1.55it/s, Loss=0.542]\n",
      "Epoch 12: 100%|██████████| 6080/6080 [1:05:33<00:00,  1.55it/s, Loss=0.483]\n",
      "Epoch 13: 100%|██████████| 6080/6080 [1:05:59<00:00,  1.54it/s, Loss=0.431]\n",
      "Epoch 14: 100%|██████████| 6080/6080 [1:05:55<00:00,  1.54it/s, Loss=0.374]\n",
      "Epoch 15: 100%|██████████| 6080/6080 [1:05:11<00:00,  1.55it/s, Loss=0.339]\n",
      "Epoch 16:  50%|█████     | 3069/6080 [33:14<32:55,  1.52it/s, Loss=0.301] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (image, mask, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     18\u001b[0m     tot\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 19\u001b[0m     image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, mask\u001b[38;5;241m.\u001b[39mto(device)        \n\u001b[0;32m     20\u001b[0m     mask_pred \u001b[38;5;241m=\u001b[39m net(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m crit(mask_pred, mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "epochs = 40\n",
    "validate = True # set to validate also during training\n",
    "loss_values = []\n",
    "macro_IoU = []\n",
    "weighted_IoU = []\n",
    "checkpoint_directory = \"D:\\\\Models\\\\Tunet1\\\\checkpoints\"\n",
    "if not Path(checkpoint_directory).is_dir():\n",
    "    print(\"Please provide a valid directory to save checkpoints in.\")\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        tot = 0\n",
    "        pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch}')\n",
    "        net.train()\n",
    "        for batch_index, (image, mask, _) in enumerate(train_loader):\n",
    "            tot+=1\n",
    "            image, mask = image.to(device), mask.to(device)        \n",
    "            mask_pred = net(image).to(device)\n",
    "            loss = crit(mask_pred, mask)\n",
    "            cumulative_loss += loss.item()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Loss': cumulative_loss/tot})\n",
    "        pbar.close()\n",
    "        loss_values.append(cumulative_loss/tot)\n",
    "        if validate:\n",
    "            # run evaluation!\n",
    "            # 1) Re-initialize data loaders\n",
    "            valid_base_sampler = SubsetRandomSampler(val_base_indices)\n",
    "            validation_base_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_base_sampler)\n",
    "            # 2) Call evaluation Loop\n",
    "            macro, weighted = eval_model(net, validation_base_loader, len(val_base_indices),show_progress=False, write_output=False)\n",
    "            # 3) Append results to list    \n",
    "            macro_IoU.append(macro)    \n",
    "            weighted_IoU.append(weighted)\n",
    "        if (epoch+1) % 2 == 0: # save checkpoint every 2 epochs\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                }, \"{}\\\\checkpoint{}\".format(checkpoint_directory, (epoch+1)))\n",
    "    print(\"Training Done!\")\n",
    "    plt.plot(loss_values)\n",
    "    plt.show()\n",
    "    #TODO:  better to save stuff in a csv file\n",
    "    with open(\"weighted_IoU.txt\", \"w\") as f:\n",
    "        for value in weighted_IoU:\n",
    "            f.write(str(value)+\"\\n\")\n",
    "    with open(\"loss.txt\", \"w\") as f:\n",
    "        for value in loss:\n",
    "            f.write(str(value)+\"\\n\")\n",
    "    with open(\"macro_IoU.txt\", \"w\") as f:\n",
    "        for value in macro_IoU:\n",
    "            f.write(str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utils (save/load/evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model evaluation, write output to file.\n",
    "# call validation function to evaluate model. Remember to re-initialize loader and sampler.\n",
    "valid_noisy_sampler = SubsetRandomSampler(val_noisy_indices)\n",
    "validation_noisy_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_noisy_sampler)\n",
    "macro,weighted = eval_model(net, validation_noisy_loader, len(val_base_indices),show_progress=True, write_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entire model (inference)\n",
    "net = torch.load(\"D:\\\\Models\\\\urnet3.3\\\\urnet3_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint (to resume training)\n",
    "checkpoint = torch.load(\"D:\\\\Models\\\\urnet3\\\\checkpoint\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run to save model in the speicifed path\n",
    "# SAVE model (only weights). Save locally or on a gitignore directory\n",
    "torch.save(net, \"D:\\\\Models\\\\urnet3.3\\\\urnet3_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write loss values on file\n",
    "with open(\"weighted_IoU.txt\", \"w\") as f:\n",
    "    for value in weighted_IoU:\n",
    "        f.write(str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training checkpoint (optimizer state as well.)\n",
    "torch.save({\n",
    "            'epoch': 150,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': 0.0435,\n",
    "            }, \"D:\\\\Models\\\\urnet3.3\\\\checkpoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
