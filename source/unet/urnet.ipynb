{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels: int, out_channels: int, padding=0):\n",
    "\treturn nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=padding)\n",
    "\n",
    "\n",
    "def max_pool_2d():\n",
    "\treturn nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncodeLayer(nn.Module):\n",
    "    # just a standard convolution layer.\n",
    "\tdef __init__(self, in_channels: int, out_channels: int, activated=True,max_pool=False, padding=0):\n",
    "\t\tsuper(UnetEncodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "            conv3x3(in_channels, out_channels, padding=padding),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "        ]\n",
    "\t\tif activated:\n",
    "\t\t\tlayers += [nn.ReLU()]\n",
    "\t\tif max_pool:\n",
    "\t\t\tlayers += [max_pool_2d()]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\t\n",
    "class UnetUpscaleLayer(nn.Module):\n",
    "\tdef __init__(self, scale_factor, in_channels):\n",
    "\t\tsuper(UnetUpscaleLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tnn.Upsample(scale_factor = (scale_factor,scale_factor), mode = 'bilinear'),\n",
    "\t\t\tconv3x3(in_channels, in_channels//2,padding=1)\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\n",
    "class UnetForwardDecodeLayer(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, padding=0):\n",
    "\t\tsuper(UnetForwardDecodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tconv3x3(in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tconv3x3(in_channels=out_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Urnet(nn.Module):\t\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Urnet, self).__init__()\n",
    "\t\tself.residuals = []\n",
    "    \t# encoding part of the Unet vanilla architecture\n",
    "\t\tself.encode1 = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=1),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=1), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode3 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=1),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode4 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=1),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode5 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=1),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=1),\n",
    "\t\t)\n",
    "\t\tself.upscale1 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 1024)\n",
    "\t\t)\n",
    "\t\tself.decode_forward1 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(1024,512, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale2 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 512)\n",
    "\t\t)\n",
    "\t\tself.decode_forward2 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(512, 256, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale3 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,256)\n",
    "\t\t)\n",
    "\t\tself.decode_forward3 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(256,128,padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale4 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,128)\n",
    "\t\t)\n",
    "\t\tself.decode_forward4 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(128,64, padding=1),\n",
    "\t\t\tnn.Conv2d(64, 6, kernel_size=1) # final conv 1x1\n",
    "\t\t\t# Model output is 6xHxW, so we have a prob. distribution\n",
    "\t\t\t# for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "\t\t)\t\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\tself.x1 = self.encode1(x)\n",
    "\t\tself.x2 = self.encode2(self.x1)\n",
    "\t\tself.x3 = self.encode3(self.x2)\n",
    "\t\tself.x4 = self.encode4(self.x3)\n",
    "\t\tself.x5 = self.encode5(self.x4)\n",
    "\n",
    "\t\ty1 = self.upscale1(self.x5)\n",
    "\t\tc1 = torch.concat((self.x4, y1), 1)\n",
    "\t\ty2 = self.decode_forward1(c1)\n",
    "\t\t\n",
    "\t\ty2 = self.upscale2(y2)\n",
    "\t\tc2 = torch.concat((self.x3, y2), 1)\n",
    "\t\ty3 = self.decode_forward2(c2)\n",
    "\n",
    "\t\ty3 = self.upscale3(y3)\n",
    "\t\tc3 = torch.concat((functional.center_crop(y3, 150), self.x2), 1)\n",
    "\t\ty4 = self.decode_forward3(c3)\n",
    "\n",
    "\t\ty4 = self.upscale4(y4)\n",
    "\t\tc4 = torch.concat((self.x1, y4), 1)\n",
    "\t\tsegmap = self.decode_forward4(c4)\n",
    "\t\treturn segmap\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class PostDamDataset(Dataset):\n",
    "\tdef __init__(self, img_dir, masks_dir):\n",
    "\t\tself.idir = img_dir\n",
    "\t\tself.mdir = masks_dir\n",
    "\t\tself.data = {} # index : (image, mask)\n",
    "\t\tself.color_to_label = {\n",
    "\t\t\t(1, 1, 0): 0,  # Yellow (cars)\n",
    "\t\t\t(0, 1, 0): 1, # Green (trees)\n",
    "\t\t\t(0, 0, 1): 2, # Blue (buildings)\n",
    "\t\t\t(1, 0, 0): 3,  # Red (clutter)\n",
    "\t\t\t(1, 1, 1): 4, # White(impervious surface),\n",
    "\t\t\t(0, 1, 1): 5 # Aqua (low vegetation)\n",
    "    \t}\n",
    "\tdef iconvert(self, mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the class label mask needed by CrossEntropy Function\n",
    "\t\tto the original mask.\n",
    "\t\tinput: class label mask, HxW\n",
    "\t\toutput: original mask, HxWx3\n",
    "\t\t\"\"\"\n",
    "\t\tH,W = mask.shape\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys())).type(torch.float64)\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values())).type(torch.float64)\n",
    "\t\toutput = torch.ones(H,W,3).type(torch.float64)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (mask == label)\n",
    "\t\t\toutput[match] = color\n",
    "\t\treturn output\n",
    "\n",
    "\tdef convert(self,mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the RGB (3x300x300) mask into a \n",
    "\t\t'class label mask' needed when computing the loss function.\n",
    "\t\tIn this new representation for each pixel we have a value\n",
    "\t\tbetween [0,C) where C is the number of classes, so 6 in this case.\n",
    "\t\tThis new tensor will have shape 1x300x300.\n",
    "\t\t\"\"\"\t\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys()))\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values()))\n",
    "\t\treshaped_mask = mask.permute(1, 2, 0).reshape(-1, 3)\n",
    "\t\tclass_label_mask = torch.zeros(reshaped_mask.shape[0], dtype=torch.long)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (reshaped_mask == color.type(torch.float64)).all(dim=1)\n",
    "\t\t\tclass_label_mask[match] = label\n",
    "\t\tclass_label_mask = class_label_mask.reshape(mask.shape[1], mask.shape[2])\t\t\n",
    "\t\treturn class_label_mask\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\titems = os.listdir(self.idir)\n",
    "\t\tfiles = [item for item in items if os.path.isfile(os.path.join(self.idir, item))]\n",
    "\t\treturn len(files)\t\t\n",
    "\tdef __getitem__(self, idx):\t\t\t\n",
    "\t\timg_path = os.path.join(self.idir, \"Image_{}.tif\".format(idx))\n",
    "\t\tmask_path = os.path.join(self.mdir, \"Label_{}.tif\".format(idx))\n",
    "\t\ttif_img = Image.open(img_path)\n",
    "\t\ttif_mask = Image.open(mask_path)\t\t\n",
    "\t\treturn (ToTensor()(tif_img), self.convert(ToTensor()(tif_mask)), idx)\n",
    "\n",
    "# dataset = PostDamDataset(\"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Images\", \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Labels\")\n",
    "# image,mask,index = dataset.__getitem__(841)\n",
    "# original_mask = dataset.iconvert(mask)\n",
    "# plt.imshow(original_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network on NVIDIA GeForce GTX 1060 6GB\n",
      "Number of parameters : 34525446\n",
      "Dataset length: 2400\n"
     ]
    }
   ],
   "source": [
    "#dataset = PostDamDataset(\"/content/drive/MyDrive/Postdam/Images\", \"/content/drive/MyDrive/Postdam/Labels\")\n",
    "# Here you need to specify dataset path for both images and labels.\n",
    "dataset = PostDamDataset(\"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Images\", \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Labels\")\n",
    "assert torch.cuda.is_available(), \"Notebook is not configured properly!\"\n",
    "device = 'cuda:0'\n",
    "print(\"Training network on {}\".format(torch.cuda.get_device_name(device=device)))\n",
    "net = Urnet().to(device)\n",
    "num_params = sum([np.prod(p.shape) for p in net.parameters()])\n",
    "print(f\"Number of parameters : {num_params}\")\n",
    "print(\"Dataset length: {}\".format(dataset.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZATION\n",
    "batch_size = 4\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "# this code chooses randomly the training and validation partitions among\n",
    "# the 2400 images, but since the random_seed is fixed, at each run it always\n",
    "# chooses the same partition\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "#for validation loader batch size is default, so 1.\n",
    "validation_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_sampler)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL\n",
    "net = torch.load(\"D:\\\\Models\\\\urnet2\\\\urnet2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP.\n",
    "epochs = 150\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    tot = 0\n",
    "    pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch}')\n",
    "    net.train()\n",
    "    for batch_index, (image, mask, _) in enumerate(train_loader):\n",
    "        tot+=1\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        mask_pred = net(image)\n",
    "        loss = crit(mask_pred, mask)\n",
    "        cumulative_loss += loss.item()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'Loss': cumulative_loss/tot})\n",
    "    pbar.close()\n",
    "    loss_values.append(cumulative_loss/tot)\n",
    "print(\"Training Done!\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION LOOP\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "print(len(validation_loader.dataset))\n",
    "tot_IoU_score = 0\n",
    "pbar = tqdm(total=len(val_indices))\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for i, (x,y, index) in enumerate(validation_loader):        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = net(x)\n",
    "        x_ref = x.cpu()\n",
    "        y_pred = y_pred.squeeze().cpu()\n",
    "        _,pred_mask = torch.max(y_pred, dim=0)\n",
    "        fig ,axarr = plt.subplots(1,3)\n",
    "        _,target_transformed_mask,_ = dataset.__getitem__(index.item())\n",
    "\n",
    "        lbl = pred_mask.cpu().numpy().reshape(-1)\n",
    "        target = y.cpu().numpy().reshape(-1)        \n",
    "        tot_IoU_score += jsc(target,lbl, average='weighted') # takes into account label imbalance\n",
    "        #tot_IoU_score += jsc(target,lbl, average='macro') # simple mean over each class.\n",
    "\n",
    "        axarr[0].title.set_text('Original Image')\n",
    "        axarr[0].imshow(x_ref.squeeze().swapaxes(0,2).swapaxes(0,1))\n",
    "\n",
    "        axarr[1].title.set_text('Model Output')\n",
    "        axarr[1].imshow(dataset.iconvert(pred_mask))\n",
    "\n",
    "        axarr[2].title.set_text('Original Mask')\n",
    "        axarr[2].imshow(dataset.iconvert(target_transformed_mask))\n",
    "        \n",
    "        pbar.update(1)\n",
    "        plt.savefig(\"output\\\\o{}.png\".format(i))\n",
    "        plt.close(fig)\n",
    "print(\"Average IoU score: {}\".format(tot_IoU_score / len(val_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run to save model in the speicifed path\n",
    "# SAVE model (only weights). Save locally or on a gitignore directory\n",
    "# torch.save(net, \"C:\\\\Users\\\\eros\\\\Desktop\\\\Models\\\\urnet3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write loss values on file\n",
    "with open(\"loss.txt\", \"w\") as f:\n",
    "    for value in loss_values:\n",
    "        f.write(str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training checkpoint (optimizer state as well.)\n",
    "torch.save({\n",
    "            'epoch': 150,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': 0.079,\n",
    "            }, \"C:\\\\Users\\\\eros\\\\Desktop\\\\Models\\\\urnet2\\\\checkpoint1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
