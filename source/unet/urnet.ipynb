{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels: int, out_channels: int, padding=0):\n",
    "\treturn nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=padding)\n",
    "\n",
    "\n",
    "def max_pool_2d():\n",
    "\treturn nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net most important blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncodeLayer(nn.Module):\n",
    "    # just a standard convolution layer.\n",
    "\tdef __init__(self, in_channels: int, out_channels: int, activated=True,max_pool=False, padding=0):\n",
    "\t\tsuper(UnetEncodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "            conv3x3(in_channels, out_channels, padding=padding),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "        ]\n",
    "\t\tif activated:\n",
    "\t\t\tlayers += [nn.ReLU()]\n",
    "\t\tif max_pool:\n",
    "\t\t\tlayers += [max_pool_2d()]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\t\n",
    "class UnetUpscaleLayer(nn.Module):\n",
    "\tdef __init__(self, scale_factor, in_channels):\n",
    "\t\tsuper(UnetUpscaleLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tnn.Upsample(scale_factor = (scale_factor,scale_factor), mode = 'bilinear'),\n",
    "\t\t\tconv3x3(in_channels, in_channels//2,padding=1)\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)\n",
    "\n",
    "class UnetForwardDecodeLayer(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, padding=0):\n",
    "\t\tsuper(UnetForwardDecodeLayer, self).__init__()\n",
    "\t\tlayers = [\n",
    "\t\t\tconv3x3(in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tconv3x3(in_channels=out_channels, out_channels=out_channels, padding=padding),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t]\n",
    "\t\tself.layer = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net base structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Urnet(nn.Module):\t\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Urnet, self).__init__()\n",
    "\t\tself.residuals = []\n",
    "    \t# encoding part of the Unet vanilla architecture\n",
    "\t\tself.encode1 = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=1),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=1), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode3 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=1),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode4 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=1),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode5 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=1),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=1),\n",
    "\t\t)\n",
    "\t\tself.upscale1 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 1024)\n",
    "\t\t)\n",
    "\t\tself.decode_forward1 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(1024,512, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale2 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2, 512)\n",
    "\t\t)\n",
    "\t\tself.decode_forward2 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(512, 256, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale3 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,256)\n",
    "\t\t)\n",
    "\t\tself.decode_forward3 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(256,128,padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale4 = nn.Sequential(\n",
    "\t\t\tUnetUpscaleLayer(2,128)\n",
    "\t\t)\n",
    "\t\tself.decode_forward4 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(128,64, padding=1),\n",
    "\t\t\tnn.Conv2d(64, 6, kernel_size=1) # final conv 1x1\n",
    "\t\t\t# Model output is 6xHxW, so we have a prob. distribution\n",
    "\t\t\t# for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "\t\t)\t\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\tself.x1 = self.encode1(x)\n",
    "\t\tself.x2 = self.encode2(self.x1)\n",
    "\t\tself.x3 = self.encode3(self.x2)\n",
    "\t\tself.x4 = self.encode4(self.x3)\n",
    "\t\tself.x5 = self.encode5(self.x4)\n",
    "\n",
    "\t\ty1 = self.upscale1(self.x5)\n",
    "\t\tc1 = torch.concat((self.x4, y1), 1)\n",
    "\t\ty2 = self.decode_forward1(c1)\n",
    "\t\t\n",
    "\t\ty2 = self.upscale2(y2)\n",
    "\t\tc2 = torch.concat((self.x3, y2), 1)\n",
    "\t\ty3 = self.decode_forward2(c2)\n",
    "\n",
    "\t\ty3 = self.upscale3(y3)\n",
    "\t\tc3 = torch.concat((functional.center_crop(y3, 150), self.x2), 1)\n",
    "\t\ty4 = self.decode_forward3(c3)\n",
    "\n",
    "\t\ty4 = self.upscale4(y4)\n",
    "\t\tc4 = torch.concat((self.x1, y4), 1)\n",
    "\t\tsegmap = self.decode_forward4(c4)\n",
    "\t\treturn segmap\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter:\n",
    "\tdef __init__(self):\n",
    "\t\tself.color_to_label = {\n",
    "            (1, 1, 0): 0,  # Yellow (cars)\n",
    "            (0, 1, 0): 1, # Green (trees)\n",
    "            (0, 0, 1): 2, # Blue (buildings)\n",
    "            (1, 0, 0): 3,  # Red (clutter)\n",
    "            (1, 1, 1): 4, # White(impervious surface),\n",
    "            (0, 1, 1): 5 # Aqua (low vegetation)\n",
    "        }\n",
    "\tdef iconvert(self, mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the class label mask needed by CrossEntropy Function\n",
    "\t\tto the original mask.\n",
    "\t\tinput: class label mask, HxW\n",
    "\t\toutput: original mask, HxWx3\n",
    "\t\t\"\"\"\n",
    "\t\tH,W = mask.shape\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys())).type(torch.float64)\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values())).type(torch.float64)\n",
    "\t\toutput = torch.ones(H,W,3).type(torch.float64)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (mask == label)\n",
    "\t\t\toutput[match] = color\n",
    "\t\treturn output\n",
    "\tdef convert(self,mask):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction needed to convert the RGB (Nx3x300x300) mask into a \n",
    "\t\t'class label mask' needed when computing the loss function.\n",
    "\t\tIn this new representation for each pixel we have a value\n",
    "\t\tbetween [0,C) where C is the number of classes, so 6 in this case.\n",
    "\t\tThis new tensor will have shape Nx300x300.\n",
    "\t\t\"\"\"\t\t\t\n",
    "\t\tC,H,W = mask.shape\n",
    "\t\tcolors = torch.tensor(list(self.color_to_label.keys()))\n",
    "\t\tlabels = torch.tensor(list(self.color_to_label.values()))\n",
    "\t\treshaped_mask = mask.permute(1, 2, 0).reshape(-1, 3)\n",
    "\t\tclass_label_mask = torch.zeros(reshaped_mask.shape[0], dtype=torch.long)\n",
    "\t\tfor color, label in zip(colors, labels):\n",
    "\t\t\tmatch = (reshaped_mask == color.type(torch.float64)).all(dim=1)\n",
    "\t\t\tclass_label_mask[match] = label\n",
    "\t\tclass_label_mask = class_label_mask.reshape(H,W)\t\t\n",
    "\t\treturn class_label_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as v2\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "class PostDamDataset(Dataset):\n",
    "\tdef __init__(self, img_dir, masks_dir,transforms=None):\n",
    "\t\tself.idir = img_dir\n",
    "\t\tself.mdir = masks_dir\n",
    "\t\tself.data = {} # index : (image, mask)\n",
    "\t\tself.transforms = transforms\t\t\n",
    "\t\tself.items = os.listdir(self.idir)\n",
    "\t\tself.files = [item for item in self.items if os.path.isfile(os.path.join(self.idir, item))]\n",
    "\t\tself.c = Converter()\n",
    "\t\tpbar = tqdm(total=len(self.files), desc='Loading dataset...')\n",
    "\t\tfor idx in range(len(self.files)):\t\t\t\n",
    "\t\t\timg_path = os.path.join(self.idir, \"Image_{}.tif\".format(idx))\n",
    "\t\t\tmask_path = os.path.join(self.mdir, \"Label_{}.tif\".format(idx))\n",
    "\t\t\ttif_img = Image.open(img_path)\n",
    "\t\t\ttif_mask = Image.open(mask_path)\t\n",
    "\t\t\tif self.transforms:  # if transforms are provided, apply them\n",
    "\t\t\t\tfinal_image = self.transforms(ToTensor()(tif_img))\n",
    "\t\t\t# no transform is applied on mask obv.\n",
    "\t\t\telse:\n",
    "\t\t\t\tfinal_image = ToTensor()(tif_img)\n",
    "\t\t\tself.data[idx] = (final_image, self.c.convert(ToTensor()(tif_mask)), idx)\n",
    "\t\t\tpbar.update(1)\n",
    "\t\tpbar.close()\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.files)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.data[idx]\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.GaussianBlur(kernel_size=(15), sigma=5),\n",
    "    v2.ElasticTransform(alpha=200.0)    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset...: 100%|██████████| 2400/2400 [00:15<00:00, 151.55it/s]\n",
      "Loading dataset...: 100%|██████████| 2400/2400 [01:28<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network on NVIDIA GeForce GTX 1060 6GB\n",
      "Number of parameters : 34525446\n",
      "Dataset length: 4800\n"
     ]
    }
   ],
   "source": [
    "#dataset = PostDamDataset(\"/content/drive/MyDrive/Postdam/Images\", \"/content/drive/MyDrive/Postdam/Labels\")\n",
    "# Here you need to specify dataset path for both images and labels.\n",
    "# WARNING: dataset is pre-loaded in memory, so high RAM usage is expected.\n",
    "images_path = \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Images\"\n",
    "labels_path = \"C:\\\\Users\\\\eros\\\\CVCS\\\\dataset\\\\Cropped_Postdam\\\\Postdam\\\\Labels\"\n",
    "base_dataset = PostDamDataset(images_path, labels_path)\n",
    "augmented_dataset = PostDamDataset(images_path, labels_path, transforms=transforms)\n",
    "dataset = ConcatDataset([base_dataset, augmented_dataset])\n",
    "\n",
    "assert torch.cuda.is_available(), \"Notebook is not configured properly!\"\n",
    "device = 'cuda:0'\n",
    "print(\"Training network on {}\".format(torch.cuda.get_device_name(device=device)))\n",
    "net = Urnet().to(device)\n",
    "num_params = sum([np.prod(p.shape) for p in net.parameters()])\n",
    "print(f\"Number of parameters : {num_params}\")\n",
    "print(\"Dataset length: {}\".format(dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset split(augmented): 3840\n",
      "Validation dataset split: 480\n",
      "Validation dataset split(only noise): 480\n"
     ]
    }
   ],
   "source": [
    "#Dataset train/validation split according to validation split and seed.\n",
    "batch_size = 4\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "dataset_size = len(dataset) #4800\n",
    "base_indices = list(range(int(dataset_size/2)))#0...2399\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(base_indices)\n",
    "augmented_indices = [i+2400 for i in base_indices] # take coresponding augmented images\n",
    "split = int(np.floor((1-validation_split) * (dataset_size//2)))\n",
    "\n",
    "train_indices = base_indices[:split]+augmented_indices[:split]\n",
    "val_base_indices = base_indices[split:]\n",
    "val_noisy_indices = augmented_indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_base_sampler = SubsetRandomSampler(val_base_indices)\n",
    "valid_noisy_sampler = SubsetRandomSampler(val_noisy_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "#for validation loader batch size is default, so 1.\n",
    "validation_base_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_base_sampler)\n",
    "validation_noisy_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_noisy_sampler)\n",
    "\n",
    "print(f\"Train dataset split(augmented): {len(train_indices)}\")\n",
    "print(f\"Validation dataset split: {len(val_base_indices)}\")\n",
    "print(f\"Validation dataset split(only noise): {len(val_noisy_indices)}\")\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score as jsc\n",
    "def eval_model(net, validation_loader, validation_len, show_progress = False, write_output=False, prefix=\"\"):\n",
    "    # returns (macro, weighted) IoU\n",
    "    c = Converter()\n",
    "    macro = 0\n",
    "    weighted = 0\n",
    "    if show_progress:\n",
    "        pbar = tqdm(total=validation_len)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for i, (x,y, index) in enumerate(validation_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = net(x)\n",
    "            x_ref = x.cpu()\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            _,pred_mask = torch.max(y_pred, dim=0)\n",
    "\n",
    "            prediction = pred_mask.cpu().numpy().reshape(-1)\n",
    "            target = y.cpu().numpy().reshape(-1)        \n",
    "            weighted += jsc(target,prediction, average='weighted') # takes into account label imbalance\n",
    "            macro += jsc(target,prediction, average='macro') # simple mean over each class.            \n",
    "            if(write_output):\n",
    "                fig ,axarr = plt.subplots(1,3)\n",
    "                _,target_transformed_mask,_ = dataset.__getitem__(index.item())\n",
    "\n",
    "                axarr[0].title.set_text('Original Image')\n",
    "                axarr[0].imshow(x_ref.squeeze().swapaxes(0,2).swapaxes(0,1))\n",
    "\n",
    "                axarr[1].title.set_text('Model Output')\n",
    "                axarr[1].imshow(c.iconvert(pred_mask))\n",
    "\n",
    "                axarr[2].title.set_text('Original Mask')\n",
    "                axarr[2].imshow(c.iconvert(target_transformed_mask))\n",
    "                plt.savefig(\"output\\\\{}_Image{}.png\".format(prefix, i))\n",
    "                plt.close(fig)\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "    macro_score = macro / validation_len\n",
    "    weighted_score = weighted / validation_len\n",
    "    if show_progress:\n",
    "        pbar.close()\n",
    "    if(write_output):\n",
    "        print(\"Macro IoU score: {}\".format(macro_score))        \n",
    "        print(\"Weigthed IoU score: {}\".format(weighted_score))\n",
    "    return macro_score, weighted_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "validate = True # set to validate also during training\n",
    "loss_values = []\n",
    "macro_IoU = []\n",
    "weighted_IoU = []\n",
    "c = Converter()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    tot = 0\n",
    "    pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch}')\n",
    "    net.train()\n",
    "    for batch_index, (image, mask, _) in enumerate(train_loader):\n",
    "        tot+=1\n",
    "        image, mask = image.to(device), mask.to(device)        \n",
    "        mask_pred = net(image).to(device)\n",
    "        loss = crit(mask_pred, mask)\n",
    "        cumulative_loss += loss.item()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'Loss': cumulative_loss/tot})\n",
    "    pbar.close()\n",
    "    loss_values.append(cumulative_loss/tot)\n",
    "    if validate:\n",
    "        # run evaluation!\n",
    "        # 1) Re-initialize data loaders\n",
    "        valid_base_sampler = SubsetRandomSampler(val_base_indices)\n",
    "        validation_base_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_base_sampler)\n",
    "        # 2) Call evaluation Loop\n",
    "        macro, weighted = eval_model(net, validation_base_loader, len(val_base_indices),show_progress=True, write_output=False)\n",
    "        # 3) Append results to list    \n",
    "        macro_IoU.append(macro)    \n",
    "        weighted_IoU.append(weighted)\n",
    "print(\"Training Done!\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utils (save/load/evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model evaluation, write output to file.\n",
    "# call validation function to evaluate model. Remember to re-initialize loader and sampler.\n",
    "valid_noisy_sampler = SubsetRandomSampler(val_noisy_indices)\n",
    "validation_noisy_loader = torch.utils.data.DataLoader(dataset ,sampler=valid_noisy_sampler)\n",
    "macro,weighted = eval_model(net, validation_noisy_loader, len(val_base_indices),show_progress=True, write_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entire model (inference)\n",
    "net = torch.load(\"D:\\\\Models\\\\urnet3.3\\\\urnet3_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint (to resume training)\n",
    "checkpoint = torch.load(\"D:\\\\Models\\\\urnet3\\\\checkpoint\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run to save model in the speicifed path\n",
    "# SAVE model (only weights). Save locally or on a gitignore directory\n",
    "torch.save(net, \"D:\\\\Models\\\\urnet3.3\\\\urnet3_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write loss values on file\n",
    "with open(\"weighted_IoU.txt\", \"w\") as f:\n",
    "    for value in weighted_IoU:\n",
    "        f.write(str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training checkpoint (optimizer state as well.)\n",
    "torch.save({\n",
    "            'epoch': 150,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': 0.0435,\n",
    "            }, \"D:\\\\Models\\\\urnet3.3\\\\checkpoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
