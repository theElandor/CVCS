{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nets\n",
    "import torch\n",
    "import sys\n",
    "import yaml\n",
    "import utils\n",
    "import torchvision.io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2, ToTensor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def register_extraction_hook(hook, module: torch.nn.Module):\n",
    "    return module.register_forward_hook(hook)\n",
    "    \n",
    "def plot_imgs(images, shape: tuple | None = None):\n",
    "    #subplot(r,c) provide the no. of rows and columns\n",
    "    if shape is None:\n",
    "        n_images = len(images)\n",
    "        max_img_row = 5\n",
    "        ncols = min(n_images, max_img_row)\n",
    "        nrows = math.ceil(len(images) / ncols)\n",
    "    else:\n",
    "        nrows, ncols = shape\n",
    "    f, axarr = plt.subplots(nrows,ncols, figsize=(3.5*ncols, 3*nrows)) \n",
    "    \n",
    "    # use the created array to output your multiple images\n",
    "    for img, ax in zip(images, axarr.ravel()):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "\n",
    "color_conv = {\n",
    "    0: (0,    0,    0), # unlabeled\n",
    "    1: (200,    0,  0), # industrial land\n",
    "    2: (250,    0,150), # urban residential\n",
    "    3: (200, 150, 150), # rural residential\n",
    "    4: (250, 150, 150), # traffic land\n",
    "    5: (0,     200, 0), # paddy field\n",
    "    6: (150,  250,  0), # irrigated cropland\n",
    "    7: (150, 200, 150), # dry cropland\n",
    "    8: (200,   0, 200), # garden plot\n",
    "    9: (150,   0, 250), # arbor woodland\n",
    "    10:(150,  150,250),  # shrub land\n",
    "    11:(250,  200,  0),  # natural grass land\n",
    "    12:(200,  200,  0),  # artificial grass land\n",
    "    13:(0,     0, 200),  # river\n",
    "    14:(0,   150, 200),  # lake\n",
    "    15:(0,   200, 250)  # pond\n",
    "}\n",
    "\n",
    "def mask_to_color(mask):\n",
    "    global color_conv\n",
    "    out = torch.zeros((3, mask.shape[0], mask.shape[1]), dtype=torch.uint8)\n",
    "    for r in range(mask.shape[0]):\n",
    "        for c in range(mask.shape[1]):\n",
    "            out[:, r, c] = torch.asarray(color_conv[mask[r,c].item()])\n",
    "    return out\n",
    "\n",
    "def class_hist(mask, nclasses):\n",
    "    out = torch.zeros(nclasses)\n",
    "    for cl in range(nclasses):\n",
    "        out[cl] = torch.count_nonzero(mask == cl) / mask.numel()\n",
    "    return out\n",
    "\n",
    "\n",
    "import torchmetrics.functional.segmentation\n",
    "import torchmetrics.functional.classification\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def per_image_jaccard(target_mask_idx, query_masks_idx):\n",
    "    result = torch.zeros(query_masks_idx.shape[0])\n",
    "    for i in range(query_masks_idx.shape[0]):\n",
    "        result[i] = torchmetrics.functional.classification.multiclass_jaccard_index(target_mask_idx, query_masks_idx[i].unsqueeze(0), 25, 'weighted', 0)\n",
    "    return result\n",
    "\n",
    "def pixel_precision(target_mask_idx, query_masks_idx):\n",
    "    return torch.sum((query_masks_idx == target_mask_idx) * (target_mask_idx != 0), dim=(-1,-2)) / target_mask_idx.count_nonzero()\n",
    "\n",
    "\n",
    "def per_image_emd(target_mask_idx, query_masks_idx, nbins):\n",
    "    # based on the work in https://arxiv.org/abs/1611.05916\n",
    "    def mask_to_norm_hist(mask):\n",
    "        mask = mask.type(torch.float32)\n",
    "        hist = torch.histc(mask, bins=nbins-1, min=1, max=nbins)\n",
    "        hist = hist.div(torch.sum(hist))\n",
    "        return hist.numpy()\n",
    "\n",
    "    target_sig = mask_to_norm_hist(target_mask_idx)\n",
    "    cs_ts = target_sig.cumsum()\n",
    "    result = np.empty(query_masks_idx.shape[0])\n",
    "    for i in range(query_masks_idx.shape[0]):\n",
    "        query_sig = mask_to_norm_hist(query_masks_idx[i])\n",
    "        # Ad-hoc formulation has been found to match opencv's results\n",
    "        # bins = np.arange(start=1, stop=nbins, dtype=np.float32)\n",
    "        # target_sig = target_sig.astype(np.float32)\n",
    "        # query_sig = query_sig.astype(np.float32)\n",
    "        # result[i] = cv.EMD(np.hstack([target_sig.reshape(-1,1), bins.reshape(-1,1)]), np.hstack([query_sig.reshape(-1,1), bins.reshape(-1,1)]), cv.DIST_L1)[0]\n",
    "        result[i] = np.sum(np.abs(cs_ts - query_sig.cumsum()))\n",
    "    return result\n",
    "    \n",
    "def compute_scores(targ_mask_idx, query_masks_idx, num_classes):\n",
    "    scores = {}\n",
    "    scores['miou'] = torchmetrics.functional.segmentation.mean_iou(targ_mask_idx.expand((query_masks_idx.shape[0], -1, -1)), query_masks_idx, num_classes, False)\n",
    "    scores['wiou'] = per_image_jaccard(targ_mask_idx, query_masks_idx)\n",
    "    scores['pprec'] = pixel_precision(targ_mask_idx, query_masks_idx)\n",
    "    scores['emd'] = per_image_emd(targ_mask_idx, query_masks_idx, num_classes)\n",
    "    return scores\n",
    "\n",
    "def scores_table(sc, n_img):\n",
    "    table = PrettyTable(0)\n",
    "    table.field_names = [\"score\"] + [f\"query{i}\" for i in range(n_img)]\n",
    "    for idx, key in enumerate(sc):\n",
    "        table.add_row([key] + sc[key].tolist())\n",
    "    print(table)\n",
    "\n",
    "def get_best_match_idx(selected_score, scores):\n",
    "    if selected_score == 'miou' or selected_score == 'wiou' or selected_score == 'pprec':\n",
    "        selector = torch.argmax\n",
    "    elif selected_score == 'emd':\n",
    "        selector = torch.argmin\n",
    "    else:\n",
    "        raise ValueError(f\"'{selected_score}' is not a valid metric\")\n",
    "    return selector(scores[selected_score])\n",
    "\n",
    "def plot_best_match_by_score(targ, quer, targ_m_color, quer_m_color, selected_score, scores):\n",
    "    best_el = get_best_match_idx(selected_score, scores)\n",
    "    plot_imgs([targ, quer[best_el], targ_m_color, quer_m_color[best_el]], (2,2))\n",
    "\n",
    "import math   \n",
    "def visualize_activations(input_act, layer_name=\"\"):\n",
    "    input_act = input_act.numpy()\n",
    "    nch, m_h, m_w = input_act.shape\n",
    "    figscale_factor = 512\n",
    "    if nch > figscale_factor:\n",
    "        figscale = math.ceil(nch / figscale_factor)\n",
    "    else:\n",
    "        figscale = 1\n",
    "    #Create figure and axes\n",
    "    fig = plt.figure(figsize=(figscale*12, figscale*8))\n",
    "        \n",
    "    #Set up title handling negative layer indexings\n",
    "    fig.suptitle(\"Activation maps for '\" + layer_name +\n",
    "                 \"'\\nLayer shape: \" + str(input_act.shape), fontsize=12*figscale)\n",
    "    # if layer_idx >= 0:\n",
    "    #     fig.suptitle(\"Activations for '\" + layer_name + \"' \" \n",
    "    #                  + layer_typename + \" layer (\" + str(layer_idx) + \"/\" \n",
    "    #                  + str(len(layers_activation_maps) - 1) \n",
    "    #                  + \")\\nLayer shape: \" + str(layer_dims[1:]),\n",
    "    #                  fontsize = 12 * figscale\n",
    "    #                 )\n",
    "    # else:\n",
    "    #     fig.suptitle(\"Activations for '\" + layer_name + \"' \" \n",
    "    #                  + layer_typename + \" layer (\" \n",
    "    #                  + str(len(layers_activation_maps) \n",
    "    #                  + layer_idx) + \"/\" + str(len(layers_activation_maps) - 1) \n",
    "    #                  + \") (idx = \" + str(layer_idx) \n",
    "    #                  + \")\\nLayer shape: \" + str(layer_dims[1:]),\n",
    "    #                  fontsize = 12 * figscale\n",
    "    #                 )\n",
    "                     \n",
    "    #Plot each 2D activation map channel in grid for selected layer          \n",
    "    #Calculate the number of rows and columns needed to arrange\n",
    "    #the activation maps into a nearly-square grid. The number of \n",
    "    #maps is the number of channels in the convolutional layer output\n",
    "    col_size = math.ceil(nch ** 0.5)\n",
    "    row_size = math.ceil(nch / col_size)\n",
    "\n",
    "    #Get image size of each channel activation map\n",
    "    act_map_shape = (m_h, m_w)\n",
    "\n",
    "    #Create a blank grid image with borders\n",
    "    border_thickness = 1\n",
    "    grid_image_shape = (\n",
    "        row_size * (act_map_shape[0] + border_thickness) + border_thickness,\n",
    "        col_size * (act_map_shape[1] + border_thickness) + border_thickness,\n",
    "    )\n",
    "\n",
    "    #Initialise image background\n",
    "    grid_image = np.empty(grid_image_shape, dtype=np.float32)\n",
    "    grid_image[:] = np.nan\n",
    "\n",
    "    #Place images in the grid\n",
    "    for ch_idx in range(nch):\n",
    "        #Get row and column coordinates\n",
    "        row = ch_idx // col_size\n",
    "        col = ch_idx % col_size\n",
    "        #Set start coordinates of region of grid image to update\n",
    "        x = col * (act_map_shape[1] + border_thickness) + border_thickness\n",
    "        y = row * (act_map_shape[0] + border_thickness) + border_thickness\n",
    "        #Update grid image values with activation map\n",
    "        grid_image[y : y + act_map_shape[0],\n",
    "                x : x + act_map_shape[1]] = input_act[ch_idx, :, :]\n",
    "    #Map NaNs to black to form borders between activation maps\n",
    "    cmap = plt.cm.viridis\n",
    "    cmap.set_bad('black', 1.)\n",
    "\n",
    "    #Turn off x-ticks\n",
    "    plt.xticks([],[])\n",
    "\n",
    "    #Add y-ticks labelling rows with their activation map channel ranges\n",
    "    # Generate labels\n",
    "    ytick_labels = [str(i*col_size) + \" - \" + str(i*col_size + row_size - 1) \n",
    "                    for i in range(col_size - 1)] \\\n",
    "                 + [str((row_size - 1) * col_size) + \" - \" + str(nch - 1)]\n",
    "    #Generate locations\n",
    "    ytick_locs = [i*(grid_image_shape[1] // len(ytick_labels)) \n",
    "                  + 0.5 * (act_map_shape[1] + border_thickness)\n",
    "                  for i in range(len(ytick_labels))]\n",
    "\n",
    "    #Set y-tick locations and labels\n",
    "    plt.yticks(ytick_locs, ytick_labels, fontsize = 10 * figscale)\n",
    "    plt.imshow(grid_image, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images\n",
    "To get some masks to work on, create a directory with one image called \"target\" (with any extension supported by PIL) and all the other images with prefix \"query\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "img_dir = \"img_directory\"\n",
    "target_fname = glob.glob(\"target*\", root_dir=img_dir)\n",
    "tensor_converter = v2.Compose([v2.PILToTensor()])\n",
    "#read base image\n",
    "target = tensor_converter(Image.open(img_dir + target_fname[0]))[:3,:,:]\n",
    "#read target images\n",
    "# query_dim = 224\n",
    "# cropper = v2.RandomCrop(query_dim)\n",
    "# big_img = tensor_converter(Image.open(target_path))[1:,:,:]\n",
    "# queries = [cropper(big_img) for _ in range(10)]\n",
    "\n",
    "query_fnames = glob.glob(\"query*\", root_dir=img_dir)\n",
    "queries = [tensor_converter(Image.open(img_dir+q))[:3,:,:] for q in query_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(target.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "net = utils.load_network({\"net\": \"Unetv2\", \"num_classes\": n_cls}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=torch.device(device))\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.eval()\n",
    "\n",
    "def take_target_features(module, args, output):\n",
    "    global target_features\n",
    "    target_features = output.clone()\n",
    "\n",
    "def take_query_features(module, args, output):\n",
    "    global query_features\n",
    "    query_features = output.clone()\n",
    "\n",
    "layer = net.encode5\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3-MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "net = utils.load_network({\"net\": \"MobileNet\", \"num_classes\": n_cls, 'backbone': 'mobilenet'}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=torch.device(device))\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.eval()\n",
    "\n",
    "def take_target_features(moudle, args, output):\n",
    "    global target_features\n",
    "    target_features = output['out'].clone()\n",
    "\n",
    "def take_query_features(module, args, output):\n",
    "    global query_features\n",
    "    query_features = output['out'].clone()\n",
    "\n",
    "layer = net.model.backbone\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3-ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "net = utils.load_network({\"net\": \"Resnet101\", \"num_classes\": n_cls}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=device)\n",
    "net.custom_load(checkpoint)\n",
    "net.eval()\n",
    "\n",
    "def take_target_features(moudle, args, output):\n",
    "    global target_features\n",
    "    target_features = output['out'].clone()\n",
    "\n",
    "def take_query_features(module, args, output):\n",
    "    global query_features\n",
    "    query_features = output['out'].clone()\n",
    "\n",
    "layer = net.model.backbone\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "net = utils.load_network({\"net\": \"SegformerMod\", \"num_classes\": n_cls}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=device)\n",
    "net.custom_load(checkpoint)\n",
    "net.eval()\n",
    "\n",
    "def take_query_features(module, args, output):\n",
    "    global query_features\n",
    "    query_features = output.last_hidden_state.clone()\n",
    "\n",
    "def take_target_features(module, args, output):\n",
    "   global target_features\n",
    "   target_features = output.last_hidden_state.clone()\n",
    "\n",
    "layer = net.segformer.segformer.encoder\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can run a forward pass with feature extraction. If you don't want to extract any features, simply \"pass\" in the body of both hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = None\n",
    "query_features = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    handler = register_extraction_hook(take_target_features, layer)\n",
    "    #input = target.unsqueeze(0).type(torch.float32)\n",
    "    target_mask = net(target.unsqueeze(0).type(torch.float32))\n",
    "    handler.remove()\n",
    "    handler = register_extraction_hook(take_query_features, layer)\n",
    "    query_masks = net(torch.stack(queries).type(torch.float32))\n",
    "    handler.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize activations\n",
    "You can visualize the produced activations by running the following code. If you want to customize the visualization itself, look in the first block of this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations(target_features.squeeze()[569:578], \"MobileNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you wanto to visualize a specific activation map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activation_channel(features, channel):\n",
    "    plt.matshow(features[channel], cmap='viridis')\n",
    "\n",
    "visualize_activation_channel(target_features.squeeze(), 820)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the final masks by argmax along output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_masks_idx = torch.argmax(query_masks, dim=1)\n",
    "target_mask_idx = torch.argmax(target_mask, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color conversion for plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_color = mask_to_color(target_mask_idx.squeeze())\n",
    "query_masks_color = [mask_to_color(mask) for mask in list(query_masks_idx[:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(target_mask_color.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(query_masks_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating similarity\n",
    "Our main goal is that of finding which query image can be most closely associated with the target image.\n",
    "We start our analysis by computing various scores between the segmentation mask of our target image and the queries.\n",
    "## Selected scores\n",
    "- **mean IoU**\n",
    "- **weighted IoU**\n",
    "- **pixel overlap**: simply the percentual amount of overlap between two segmentation masks as $$po(t,q) = \\frac{1}{N}\\sum_{s=1}^{N}\\mathbb{1}[t_s=q_s]$$\n",
    "- **Earth Mover's Distance**: EMD between segmentation mask histograms.\n",
    "\n",
    "## Some thoughts on scores and the concept of similarity in semantic segmentation\n",
    "The main question that should guide this quest on esimating similarity is: what do we mean by \"similar\" images in the case of semantic segmentation of satellite colour images? I want to start with some consideration on the metrics used. \n",
    "\n",
    "Both IoU metrics as well as pixel overlap are heavily influenced by the spatial characteristics of images (and masks). As a thought experiment, think of a shoreline, obviously observed in bird's eye view: we can easily picture a very simmetric image, with a line created where the water meets the sand; now think of the same image but flipped by 180 degrees. Sand and water \"swithced places\", but the overall content is the very same. How would such an image score with these metrics, taking the original as our ground truth? It's trivial to conclude that there would be little overlap for both classes (observe that the overlap for at least one class grows the more the separation line is distant from the center of the image). If we instead take an image where only sand (or water) is visible, this image would counterintuitively score higher than the transformed image, even though we could consider it semantically different since one object class is completely missing.\n",
    "\n",
    "EMD on the other hand completely discards spatial information, since it measures the minimal effort required to \"morph\" one distribution into another: in our case the distribution is the dense pixel classifications output by the segmentation model (which is represented using normalized histograms). Going back to the previous example, now the transformed image would be the highest scoring, since it contains the exact same pixels.\n",
    "> In fact the EMD paper defines a ground distance that considers the spatial information of a pixel plus its color...\n",
    "\n",
    "Moreover, we impose an ordering on classes, such that similar classes will be closer together. Look at [this](https://arxiv.org/abs/1611.05916) paper for more info. The bottom line is that EMD for an ordered-class, L1 ground distance matrix has a closed-form solution, so it's very easy to compute (code in the first cell of this notebook...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_scores(target_mask_idx, query_masks_idx, 16)\n",
    "scores_table(scores, len(query_masks_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best match for a specific score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'emd'\n",
    "plot_best_match_by_score(target, queries, target_mask_color, query_masks_color, score, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort images by score, then plot them and their masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_score = sorted(zip(scores[score], range(len(queries))))\n",
    "plot_imgs([queries[i[1]] for i in sorted_by_score]+[query_masks_color[i[1]] for i in sorted_by_score], (2, len(queries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also inspect activation maps for queries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations(query_features[get_best_match_idx('emd', scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works on output planes too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations(target_mask.squeeze(), \"output masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations(query_masks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring DINO ViT features for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'utils' in sys.modules:\n",
    "    sys.modules.pop('utils')\n",
    "    del utils\n",
    "dino_vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8').eval()\n",
    "dino_vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16').eval()\n",
    "dino_vitb16 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16').eval()\n",
    "dino_vitb8 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8').eval()\n",
    "if 'utils' in sys.modules:\n",
    "    sys.modules.pop('utils')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WARNING! RUN ONLY IF YOU HAVE A FINETUNED FULL CHECKPOINT (student + teacher nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuned dino here...\n",
    "\n",
    "finetuned_dino_chkp= torch.load(\"finetuned.pth\", map_location=torch.device(device))\n",
    "checkpoint_state_dict_mod = {}\n",
    "checkpoint_state_dict = finetuned_dino_chkp['student']\n",
    "for item in checkpoint_state_dict:\n",
    "    s = str(item)\n",
    "    if 'module.backbone' in s:\n",
    "        checkpoint_state_dict_mod[s.replace('module.backbone.', '')] = checkpoint_state_dict[item]\n",
    "dino_vits16.load_state_dict(checkpoint_state_dict_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vit_feats_target = dino_vits8(target.unsqueeze(0))\n",
    "    vit_feats_queries = dino_vits8(torch.stack(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DINO ViT's features have shown to be reliable for k-NN classification and copy detection. Should be able to find the most similar image by computing the L2 distance between the produced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = torch.cdist(vit_feats_queries.unsqueeze(0), vit_feats_target.unsqueeze(0)).squeeze()\n",
    "best_el_l2 = torch.argmin(l2)\n",
    "plot_imgs([target, queries[best_el_l2], target_mask_color, query_masks_color[best_el_l2]], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = torch.cosine_similarity(vit_feats_target, vit_feats_queries)\n",
    "best_el_cos = torch.argmax(cosine)\n",
    "plot_imgs([target, queries[best_el_cos], target_mask_color, query_masks_color[best_el_cos]], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up databases\n",
    "Here you can setup the two databases. Nothing fancy, just read files and put them into tensors and lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "db_dir = 'db/directory'\n",
    "query_dir = 'query/directory'\n",
    "db_files = glob.glob(db_dir+'*')\n",
    "query_files = glob.glob(query_dir+'*')\n",
    "\n",
    "# read all files to retrieve\n",
    "retr = []\n",
    "for img in db_files:\n",
    "    retr.append(torchvision.io.read_image(img)[:3,:,:])\n",
    "to_retrieve = []\n",
    "for img in query_files:\n",
    "    to_retrieve.append(torchvision.io.read_image(img)[:3,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(to_retrieve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Extract segmentation masks with one of the models you instantiated.\n",
    "To change model, just run the correspoing cell at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute masks for all these images\n",
    "with torch.no_grad():\n",
    "    retr_masks = []\n",
    "    for t in retr:\n",
    "        retr_masks.append(torch.argmax(net(t.unsqueeze(0).type(torch.float32)), dim=1))\n",
    "\n",
    "    to_retrieve_masks = []\n",
    "    for t in to_retrieve:\n",
    "        to_retrieve_masks.append(torch.argmax(net(t.unsqueeze(0).type(torch.float32)), dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing EMD\n",
    "EMD is computed taking as reference the images into the \"query\" directory. For each image, the score is computed by taking every image present in the \"db\" directory.\n",
    "Then we sort the images based on scores. For each query image, a list of tuples containing score and corresponding \"db\" image index number is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute emd for every image\n",
    "emds = []\n",
    "for m in to_retrieve_masks:\n",
    "    emds.append(per_image_emd(m, torch.stack(retr_masks), 16))\n",
    "\n",
    "sorted_emds = [sorted(zip(emds[img_idx].tolist(), range(len(retr)))) for img_idx in range(len(to_retrieve))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO embeddings\n",
    "Get dino embeddings. Running inference on single images (NO batch mode!) is a lot more memory friendly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vit embeddings\n",
    "vit = dino_vits16\n",
    "emb_retr = []\n",
    "emb_to_retrieve = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(retr)):\n",
    "        emb_retr.append(vit(retr[i].type(torch.float32).unsqueeze(0)).squeeze())\n",
    "    for i in range(len(to_retrieve)):\n",
    "        emb_to_retrieve.append(vit(to_retrieve[i].type(torch.float32).unsqueeze(0)).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the similarity with the L2 distance between the \"query\" embeddings and all \"db\" images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now l2 distance between embeddings\n",
    "l2_dist = torch.cdist(torch.stack(emb_to_retrieve), torch.stack(emb_retr))\n",
    "sorted_l2 = [sorted(zip(l2_dist[img_idx].tolist(), range(len(retr)))) for img_idx in range(len(to_retrieve))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one image and visualize the results. You can select the top N retrieved result by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your image...\n",
    "selected_img_idx = 7\n",
    "selected_img = to_retrieve[selected_img_idx]\n",
    "topN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(selected_img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs([retr[best[1]] for best in sorted_emds[selected_img_idx][:topN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs([retr[best[1]] for best in sorted_l2[selected_img_idx][:topN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_evaluation(emd, other_score):\n",
    "    top1 = 0.\n",
    "    top5 = 0.\n",
    "    top10 = 0.\n",
    "    top5to5 = 0.\n",
    "    top5to10 = 0.\n",
    "    top5to20 = 0.\n",
    "    top5to50 = 0.\n",
    "    dim = len(emd)\n",
    "    for i in range(dim):\n",
    "        emd_best5 = [el[1] for el in emd[i][0:5]]\n",
    "        l2_bests = [el[1] for el in other_score[i][0:50]]\n",
    "        top1 += (emd_best5[0] == l2_bests[0]) / dim\n",
    "        top5 += (emd_best5[0] in l2_bests[:5]) / dim\n",
    "        top10 += (emd_best5[0] in l2_bests[:10]) / dim\n",
    "        top5to5 += sum(d in l2_bests[:5] for d in emd_best5) / (5*dim)\n",
    "        top5to10 += sum(d in l2_bests[:10] for d in emd_best5) / (5*dim)\n",
    "        top5to20 += sum(d in l2_bests[:20] for d in emd_best5) / (5*dim)\n",
    "        top5to50 += sum(d in l2_bests for d in emd_best5) / (5*dim)\n",
    "\n",
    "    print(top1, top5, top10, top5to5, top5to10, top5to20, top5to50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_evaluation(sorted_emds, sorted_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GID\n",
    "| model | top1 | top5 | top10 | top5to5 | top5to10 | top5to20 | top5to50 |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| s8 | 0.1 | 0.1 | 0.2 | 0.1 | 0.16 | 0.26 | 0.48 |\n",
    "| s16 | 0.1 | 0.1 | 0.1 | 0.08 | 0.17999999999999997 | 0.23999999999999996 | 0.52 |\n",
    "| b8 | 0.1 | 0.1 | 0.2 | 0.08 | 0.14 | 0.24000000000000002 | 0.54 |\n",
    "| b16 | 0.1 | 0.1 | 0.1 | 0.06 | 0.08 | 0.15999999999999998 | 0.45999999999999996 |\n",
    "\n",
    "### ESA\n",
    "| model | top1 | top5 | top10 | top5to5 | top5to10 | top5to20 | top5to50 |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| s8 | 0.0 | 0.0 | 0.0 | 0.04 | 0.04 | 0.04 | 0.26 |\n",
    "| s16 | 0.0 | 0.0 | 0.0 | 0.02 | 0.04 | 0.08 | 0.26 |\n",
    "| b8 | 0.0 | 0.0 | 0.0 | 0.02 | 0.04 | 0.08 | 0.26 |\n",
    "| b16 | 0.0 | 0.0 | 0.0 | 0.02 | 0.06 | 0.08 | 0.26 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet backbone feature planes as retrieval embeddings\n",
    "We tried converting the backbone's feature planes into embeddings by performing a max operation on each full plane, obtaining at the end a 960-dimensional vector to be used for retrieval. L2 distance for computing distances, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"/home/pit/Desktop/mobilenetdl_nobg_aug_checkpoint64\"\n",
    "mobilenet = utils.load_network({\"net\": \"MobileNet\", \"num_classes\": n_cls, 'backbone': 'mobilenet'}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=torch.device(device))\n",
    "mobilenet.load_state_dict(checkpoint['model_state_dict'])\n",
    "mobilenet.eval()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_emb = None\n",
    "\n",
    "def take_mobilenet_emb(moudle, args, output):\n",
    "    global mobilenet_emb\n",
    "    mobilenet_emb = output['out'].squeeze().clone()\n",
    "\n",
    "layer = mobilenet.model.backbone\n",
    "\n",
    "with torch.no_grad():\n",
    "    mobilenet_emb_to_retrieve = []\n",
    "    mobilenet_emb_db = []\n",
    "    handler = register_extraction_hook(take_mobilenet_emb, layer)\n",
    "    for img in to_retrieve:\n",
    "        mobilenet(img.unsqueeze(0).type(torch.float32))\n",
    "        mobilenet_emb_to_retrieve.append(mobilenet_emb)\n",
    "    for img in retr:\n",
    "        mobilenet(img.unsqueeze(0).type(torch.float32))\n",
    "        mobilenet_emb_db.append(mobilenet_emb)\n",
    "    handler.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_retrieve = []\n",
    "feats_db = []\n",
    "\n",
    "#  max pooling on channels\n",
    "for emb in mobilenet_emb_to_retrieve:\n",
    "    feats_to_retrieve.append(torch.max(emb.flatten(start_dim=1), dim=1)[0].flatten())\n",
    "for emb in mobilenet_emb_db:\n",
    "    feats_db.append(torch.max(emb.flatten(start_dim=1), dim=1)[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_l1 = torch.cdist(torch.stack(feats_to_retrieve), torch.stack(feats_db), p=1)\n",
    "sorted_mobilenet_l1 = [sorted(zip(mobilenet_l1[img_idx].tolist(), range(len(retr)))) for img_idx in range(len(to_retrieve))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs([retr[best[1]] for best in sorted_mobilenet_l1[selected_img_idx][:topN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_evaluation(sorted_emds, sorted_mobilenet_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GID\n",
    "| dist | top1 | top5 | top10 | top5to5 | top5to10 | top5to20 | top5to50 |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| L2 | 0.1 | 0.5 | 0.7 | 0.12000000000000001 | 0.34 | 0.44 | 0.6000000000000001 |\n",
    "| L1 | 0.2 | 0.5 | 0.7 | 0.16 | 0.36 | 0.46 | 0.5800000000000001 |\n",
    "### ESA\n",
    "| dist | top1 | top5 | top10 | top5to5 | top5to10 | top5to20 | top5to50 |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| L2 | 0.1 | 0.1 | 0.2 | 0.04 | 0.06 | 0.18 | 0.49999999999999994 |\n",
    "| L1 | 0.1 | 0.1 | 0.2 | 0.06 | 0.1 | 0.22 | 0.44 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other experiments\n",
    "We tried also with segformer features, without particular success. You are free to use these cells for experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "segformer_net = utils.load_network({\"net\": \"SegformerMod\", \"num_classes\": n_cls}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=device)\n",
    "segformer_net.custom_load(checkpoint)\n",
    "segformer_net.eval()\n",
    "\n",
    "encoded_queries = None\n",
    "encoded_retr = None\n",
    "\n",
    "def take_encoded_t(module, args, output):\n",
    "    global encoded_queries\n",
    "    encoded_queries = output.last_hidden_state.clone()\n",
    "\n",
    "def take_encoded_q(module, args, output):\n",
    "   global encoded_retr\n",
    "   encoded_retr = output.last_hidden_state.clone()\n",
    "\n",
    "layer = segformer_net.segformer.segformer.encoder\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb_queries = []\n",
    "    handler = register_extraction_hook(take_encoded_t, layer)\n",
    "    for img in to_retrieve:\n",
    "        segformer_net(img.unsqueeze(0))\n",
    "        emb_queries.append(encoded_queries.squeeze())\n",
    "    handler.remove()\n",
    "    emb_retr = []\n",
    "    handler = register_extraction_hook(take_encoded_q, layer)\n",
    "    for img in retr:\n",
    "        segformer_net(img.unsqueeze(0))\n",
    "        emb_retr.append(encoded_retr.squeeze())\n",
    "    handler.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segfrm_l2 = torch.cdist(torch.stack(emb_queries).flatten(start_dim=1), torch.stack(emb_retr).flatten(start_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_segfrm_l2 = sorted(zip(segfrm_l2[selected_img_idx].tolist(), range(len(retr))))\n",
    "plot_imgs([retr[best[1]] for best in sorted_segfrm_l2[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segfrm_cos_sim = torch.nn.functional.cosine_similarity(torch.stack(emb_queries).flatten(start_dim=1).unsqueeze(1), torch.stack(emb_retr).flatten(start_dim=1).unsqueeze(0), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_segfrm_cos_sim = sorted(zip(segfrm_cos_sim[selected_img_idx].tolist(), range(len(retr))), reverse=True)\n",
    "plot_imgs([retr[best[1]] for best in sorted_segfrm_cos_sim[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cls = 15\n",
    "chkpt = \"your/checkpoint\"\n",
    "unet = utils.load_network({\"net\": \"Unetv2\", \"num_classes\": n_cls}, device)\n",
    "checkpoint = torch.load(chkpt, map_location=torch.device(device))\n",
    "unet.load_state_dict(checkpoint['model_state_dict'])\n",
    "unet.eval()\n",
    "\n",
    "def take_embedding(module, args, output):\n",
    "    global unet_embedding\n",
    "    unet_embedding = output.clone()\n",
    "\n",
    "layer = unet.encode5\n",
    "\n",
    "unet_embedding = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    unet_emb_queries = []\n",
    "    handler = register_extraction_hook(take_embedding, layer)\n",
    "    for img in to_retrieve:\n",
    "        unet(img.unsqueeze(0).type(torch.float32))\n",
    "        unet_emb_queries.append(unet_embedding.squeeze())\n",
    "    unet_emb_retr = []\n",
    "    for img in retr:\n",
    "        unet(img.unsqueeze(0).type(torch.float32))\n",
    "        unet_emb_retr.append(unet_embedding.squeeze())\n",
    "    handler.remove()\n",
    "\n",
    "unet_feats_to_retrieve = []\n",
    "unet_feats_db = []\n",
    "\n",
    "#  max pooling on channels\n",
    "for emb in unet_emb_queries:\n",
    "    unet_feats_to_retrieve.append(torch.max(emb.flatten(start_dim=1), dim=1)[0].flatten())\n",
    "for emb in unet_emb_retr:\n",
    "    unet_feats_db.append(torch.max(emb.flatten(start_dim=1), dim=1)[0].flatten())\n",
    "\n",
    "unet_l2 = torch.cdist(torch.stack(unet_feats_to_retrieve), torch.stack(unet_feats_db))\n",
    "sorted_unet_l2 = [sorted(zip(unet_l2[img_idx].tolist(), range(len(retr)))) for img_idx in range(len(to_retrieve))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_evaluation(sorted_emds, sorted_unet_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GID 0.2 0.2 0.30000000000000004 0.2 0.28 0.4 0.6400000000000001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
