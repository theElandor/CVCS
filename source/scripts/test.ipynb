{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\web\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from blocks import UnetEncodeLayer,UnetUpscaleLayer,UnetForwardDecodeLayer, conv3x3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torchvision.transforms.functional as functional\n",
    "import math\n",
    "from transformers import AutoModel, AutoImageProcessor, SegformerForSemanticSegmentation, SegformerConfig\n",
    "from blocks import UnetEncodeLayer,UnetUpscaleLayer,UnetForwardDecodeLayer\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101,deeplabv3_mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\t# D = embedding dimension (patch is p*p*3 and will be projected to be D dimensional)\n",
    "\t# N = number of patches\n",
    "\t# p = patch size\t\n",
    "\tdef __init__(self, D, num_heads):\n",
    "\t\tsuper(VisionTransformer, self).__init__()\n",
    "\t\t# self.linear_projection = nn.Linear(p*p*3, D) don't need it in this architecture\n",
    "\t\t# self.positional_encoding = PositionalEncoding(D, N) neither this.\n",
    "\t\tself.layer_norm1_1 = nn.LayerNorm(D) #q\n",
    "\t\tself.layer_norm1_2 = nn.LayerNorm(D) #k,v\n",
    "\t\tself.layer_norm2 = nn.LayerNorm(D)\n",
    "\t\tself.MHA = nn.MultiheadAttention(embed_dim=D, num_heads=num_heads, batch_first=True)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\t# using D*4 hidden size according to original vision transformer paper\n",
    "\t\t\tnn.Linear(D, D*4),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(D*4, D)\n",
    "\t\t)\n",
    "\t\t# we should have one of this for each head\n",
    "\tdef forward(self, s: torch.tensor):\n",
    "\t\t\"\"\"\n",
    "\t\tParameters:\n",
    "\t\t\t+ s <concatenation of query, key and value>\n",
    "\t\t\"\"\"\n",
    "\t\t#x = self.linear_projection(x) # N, p*p*3 --> N, D\n",
    "\t\t#self.r1 = self.positional_encoding(x) # add positional encoding to x, embedded patches\n",
    "\t\tq = s[0]\n",
    "\t\tk = s[1]\n",
    "\t\tv = s[2]\n",
    "\t\tself.r1 = q\n",
    "\t\tq = self.layer_norm1_1(q)\n",
    "\t\tk = self.layer_norm1_2(k)\n",
    "\t\tv = self.layer_norm1_2(v)\n",
    "\n",
    "\t\tx = self.MHA(q,k,v)[0]\n",
    "\n",
    "\t\tself.r2 = x + self.r1\n",
    "\t\tx = self.layer_norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\treturn torch.stack((x+self.r2, k, v), dim=0)\n",
    "\n",
    "def vision_transformer(D,num_heads):\n",
    "\treturn VisionTransformer(D,num_heads)\n",
    "\t\n",
    "class VisionTransformerEncoder(nn.Module):\n",
    "\tdef __init__(self, D, num_heads, layers):\n",
    "\t\tsuper(VisionTransformerEncoder, self).__init__()\n",
    "\t\tself.layers =[vision_transformer(D,num_heads) for _ in range(layers)]\n",
    "\t\tself.stack = nn.Sequential(*self.layers)\n",
    "\tdef forward(self, q,k,v):\n",
    "\t\ts = torch.stack((q,k,v), dim=0)\n",
    "\t\treturn self.stack(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUnet(nn.Module):\n",
    "\t  # classic Unet with some reshape and cropping to match our needs.\n",
    "\tdef __init__(self, num_classes, D=196):\n",
    "\t\tsuper(FUnet, self).__init__()\t\t\n",
    "\t\tself.requires_context = True\n",
    "\t\tself.wrapper = False\n",
    "\t\tself.returns_logits = True\n",
    "\t\t# -----------------PATCH ENCODER-----------------------\n",
    "\t\tself.D = D\n",
    "\t\tself.encode1 = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=1),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=1), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=1),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode3 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=1),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode4 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=1),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=1),\n",
    "\t\t)\n",
    "\t\tself.encode5 = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=1),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=1),\n",
    "\t\t)\n",
    "\t\t# -----------------CONTEXT ENCODER--------------------\n",
    "\t\tself.encode1_c = nn.Sequential(\n",
    "\t\t\tUnetEncodeLayer(3, 64, padding=3, dilation=3),\n",
    "\t\t\tUnetEncodeLayer(64, 64, padding=3, dilation=3), ## keep dimensions unchanged\n",
    "\t\t)\n",
    "\t\tself.encode2_c = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(64, 128, padding=3, dilation=3),\n",
    "\t\t\tUnetEncodeLayer(128, 128, padding=3, dilation=3),\n",
    "\t\t)\n",
    "\t\tself.encode3_c = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(128, 256, padding=3, dilation=3),\n",
    "\t\t\tUnetEncodeLayer(256, 256, padding=3, dilation=3),\n",
    "\t\t)\n",
    "\t\tself.encode4_c = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(256, 512, padding=3, dilation=3),\n",
    "\t\t\tUnetEncodeLayer(512, 512, padding=3, dilation=3),\n",
    "\t\t)\n",
    "\t\tself.encode5_c = nn.Sequential(\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tUnetEncodeLayer(512, 1024, padding=3, dilation=3),\n",
    "\t\t\tUnetEncodeLayer(1024, 1024, padding=3, dilation=3),\n",
    "\t\t)\t\t\n",
    "\n",
    "\t\t# ---------------DECODER-----------------\n",
    "\n",
    "\t\tself.upscale1 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(1024, 512,kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\tself.decode_forward1 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(1536,512, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale2 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(512, 256,kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\tself.decode_forward2 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(768, 256, padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale3 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(256, 128,kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\tself.decode_forward3 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(384,128,padding=1)\n",
    "\t\t)\n",
    "\t\tself.upscale4 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(128, 64,kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\tself.decode_forward4 = nn.Sequential(\n",
    "\t\t\tUnetForwardDecodeLayer(192,64, padding=1),\n",
    "\t\t\tnn.Conv2d(64, num_classes, kernel_size=1) # final conv 1x1\n",
    "\t\t\t# Model output is 6xHxW, so we have a prob. distribution\n",
    "\t\t\t# for each pixel (each pixel has a logit for each of the 6 classes.)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.transformer = VisionTransformerEncoder(self.D, 4, 2)\n",
    "\n",
    "\tdef encode_patch(self, x: torch.Tensor):\n",
    "\t\tself.x1 = self.encode1(x)\n",
    "\t\tself.x2 = self.encode2(self.x1)\n",
    "\t\tself.x3 = self.encode3(self.x2)\n",
    "\t\tself.x4 = self.encode4(self.x3)\n",
    "\t\tself.x5 = self.encode5(self.x4)\n",
    "\t\treturn self.x5\n",
    "\n",
    "\tdef encode_context(self, x:torch.Tensor):\n",
    "\t\tself.x1_c = self.encode1_c(x)\n",
    "\t\tself.x2_c = self.encode2_c(self.x1_c)\n",
    "\t\tself.x3_c = self.encode3_c(self.x2_c)\n",
    "\t\tself.x4_c = self.encode4_c(self.x3_c)\n",
    "\t\tself.x5_c = self.encode5_c(self.x4_c)\n",
    "\t\treturn self.x5_c\n",
    "\t\n",
    "\tdef embedding_fusion(self):\n",
    "\t\tN,L,h,w = self.x5.shape\n",
    "\t\tD = h*w\t\t\n",
    "\t\tq = self.x5.reshape(N,L,D)\n",
    "\t\tk = v = self.x5_c.reshape(N,L,D)\n",
    "\t\ttest = self.transformer(q,k,v)\n",
    "\t\tself.fused_features = test[0].reshape(N,L,h,w)\n",
    "\t\tpass\n",
    "\n",
    "\tdef decode(self):\n",
    "\t\ty1 = self.upscale1(self.fused_features)\n",
    "\n",
    "\t\tc1 = torch.concat((self.x4, self.x4_c, y1), 1)\n",
    "\t\ty2 = self.decode_forward1(c1)\n",
    "\t\t\n",
    "\t\ty2 = self.upscale2(y2)\n",
    "\t\tc2 = torch.concat((self.x3, self.x3_c, y2), 1)\n",
    "\t\ty3 = self.decode_forward2(c2)\n",
    "\n",
    "\t\ty3 = self.upscale3(y3)\n",
    "\t\tc3 = torch.concat((self.x2, self.x2_c, functional.center_crop(y3, self.x2.shape[2])), 1)\n",
    "\t\ty4 = self.decode_forward3(c3)\n",
    "\n",
    "\t\ty4 = self.upscale4(y4)\n",
    "\t\tc4 = torch.concat((self.x1, self.x1_c, y4), 1)\n",
    "\t\treturn self.decode_forward4(c4)\n",
    "\t\t\n",
    "\tdef forward(self, x: torch.Tensor, context):\n",
    "\t\tpatch_embedding = self.encode_patch(x)\n",
    "\t\tcontext_embedding = self.encode_context(context)\n",
    "\t\tself.embedding_fusion()\n",
    "\t\tsegmap = self.decode()\n",
    "\t\treturn segmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FUnet(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((2,3,224,224))\n",
    "out = net(test, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
